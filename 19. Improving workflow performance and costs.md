- [Improving workflow performance and costs](#improving-workflow-performance-and-costs)
  - [12.1 Dealing with high-volume builds](#121-dealing-with-high-volume-builds)
    - [12.1.1 Concurrency groups](#1211-concurrency-groups)
    - [12.1.2 Merge queues](#1212-merge-queues)
      - [How merge queues work](#how-merge-queues-work)
  - [12.2 Reducing the costs of maintaining artifacts](#122-reducing-the-costs-of-maintaining-artifacts)
  - [12.3. Improving performance](#123-improving-performance)
    - [12.3.1 Using a sparse checkout](#1231-using-a-sparse-checkout)
    - [12.3.2 Adding caching](#1232-adding-caching)
    - [12.3.3 Detecting a cache hit and skipping the work](#1233-detecting-a-cache-hit-and-skipping-the-work)
    - [12.3.4 Selecting other runners](#1234-selecting-other-runners)
      - [Speeding up builds with larger runners](#speeding-up-builds-with-larger-runners)
      - [Lowering your costs with self-hosted runners](#lowering-your-costs-with-self-hosted-runners)
  - [12.4 Optimizing your jobs](#124-optimizing-your-jobs)
  - [Summary](#summary)

# Improving workflow performance and costs

This chapter covers

* Dealing with high-volume builds
* Reducing the costs of maintaining artifacts
* Improving performance

This short and final chapter of this book will share some insights into how you can improve the performance and costs of your GitHub Actions workflows. We will first look into how we can deal with repos with a high volume of commits that need to be merged. This can incur long wait times for the integration and high costs regarding the number of minutes of build time consumed. Next, we will look into some optimizations you can implement by reducing the cost of artifacts and improving the performance of your workflows by using concepts like caching and changing the runners you use. Let us get started with high-volume repos.

## 12.1 Dealing with high-volume builds

When you have a team of developers submitting code to the repository frequently using pull requests (PRs), you might have long wait times before your changes are accepted in the main branch. This is caused by the fact that jobs take a long time to complete, which will delay the feedback. You can use two approaches to deal with the number of builds becoming larger and getting slower feedback. One option is to use concurrency groups, and the other is to use merge queues. The next sections will describe this in further detail.

### 12.1.1 Concurrency groups

One way of dealing with high-volume builds is using a feature called *concurrency groups* in your workflow definition. Concurrency groups are defined in the workflow definitions, enabling you to cancel jobs running when a new job appears with the same criteria as you specify. The second job gets queued and needs to wait. The job is canceled when another build is queued. This approach works very well when you have builds that take longer to complete (e.g., when you are in game development and optimizing and bundling assets). When it takes several minutes to complete a build and you are only interested in the latest build, you can change the workflow to group a job with a concurrency definition and set the `cancel-in-progress` option to `true`. This will result in the build being canceled, and a new build with newer content will be started. This prevents unnecessary uses of resources, since the previous build will not produce useful outcomes, and it can save on costs when running on the hosted runners.

> [!NOTE]
> 
> Using concurrency groups is not a solution for team members to push too frequently to the central repository. Please work with your team, and ensure they understand the best way of working is to commit often to your local branch, but only push the changes when your work is done or ready for review. Pushing every single change is an anti-pattern in the way of working with Git.

One other place this is very useful is when you are running deployments, and you want to cancel one that you know will fail so that you can push forward on a fix. The concurrency group can cancel the current deployment, so you expedite the process of rolling out the fix. The syntax for concurrency groups is shown in the following listing.

**Listing 12.1 Concurrency syntax**

```yml
concurrency:
  group: ${{ github.workflow }}-${{ github.refname }}
  # If enabled, this cancels the current running and starts the latest.
  cancel-in-progress: true
```

In this example, we defined the group with a unique name for this workflow per branch triggering the workflow. This way, a build from another branch will not be affected by a build on the main branch.

Note that when you set the `cancel-in-progress` option to `false` (which is the default), the only result will be that you enforce all builds to run sequentially, which will not lead to any reduction of costs. Sometimes, however, this can be useful—for example, when the workflow is accessing a resource that multiple running workflows cannot access at the same moment. This is more common in CD scenarios.

When a workflow gets canceled because the concurrency group terminates, you can see this in the logs, and you will get a notification (see figure 12.1). This way, you can differentiate between manually and automatically canceled workflows. Note that a workflow that gets canceled will not send a notification.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH12_F01_Kaufmann.png)<br>
**s.1 A canceled workflow that did not send a notification**

And if you drill down in the canceled workflow, shown in figure 12.2, you can see why it got canceled.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH12_F02_Kaufmann.png)<br>
**Figure 12.2 Reason for cancelation**

### 12.1.2 Merge queues

A second way to deal with busy branches is using *merge queues*. Using merge queues implies using a branching strategy that uses PRs to merge into the main branch. You create your feature branch, create your change, and open a PR to merge it into the main branch. When we set up a merge queue, we increase velocity by automating PR merges into a busy branch and ensuring the branch is never broken by incompatible changes. The merge queue provides the same benefits as the Require Branches to Be Up to Date Before Merging branch protection. The difference is that it does not require a PR author to update their PR branch and wait for status checks to finish before trying to merge. Using a merge queue is particularly useful on branches with many PRs merging daily from many different users.

Once a PR has passed all required branch protection checks, a user with write access to the repository can add the PR to the queue. The merge queue will ensure the PR’s changes pass all required status checks when applied to the latest version of the target branch and any PRs already in the queue.

To enforce the use of a merge queue, you need to set up a branch protection rule for the branch to which you want this to be applied—usually the main branch. This rule then needs to check the option Require Merge Queue.

For your workflows to execute on a merge queue trigger, you need to modify the workflow to contain this trigger. This is shown in the following listing.

**Listing 12.2 Using merge queue triggers**

```yml
on:
  pull_request:
  merge_group:
```

#### How merge queues work

As PRs are added to the merge queue, the merge queue ensures they are merged in a first-in, first-out order, where the required checks are always satisfied.

A merge queue creates temporary branches with a special prefix to validate PR changes. When a PR is added to the merge queue, the changes in the PR are grouped into a `merge_group` with the latest version of the `base_branch` and changes from PRs ahead of it in the queue. GitHub will merge all these changes into the `base_branch` once the checks required by the branch protections of `base_branch` pass.

## 12.2 Reducing the costs of maintaining artifacts

When you upload the artifacts to the artifact store, you will occupy storage, for which you need to pay. Artifacts are retained for 90 days by default. You can specify a shorter retention period using the retention-days input, as shown in the following listing.

**Listing 12.3 Shorter retention period**

```yml
- uses: actions/upload-artifact
  with:
    name: my-artifact
    path: ./my_path
    retention-days: 30
```

The amount you pay depends on the license you have from GitHub. But, in general, it is a good idea to limit the amount of storage you use and not get a bill you did not expect. You don’t want to pay for artifacts you are not using anymore. Therefore, it is helpful that you can set the retention time on the artifacts you create. A general rule of thumb is to look at your deployments: if you deploy multiple times a week and have a “roll-forward” strategy, you do not need to store the artifacts for more than a couple of days. When your artifacts get deployed to either packages or releases, you can already remove them, since you do not need them as artifacts anymore.

## 12.3. Improving performance

Until now, we have not put any additional effort into improving the speed of running our workflows. A few options can help us speed up the run of a workflow. The two main options are caching and selecting other types of runners. Let’s look at both options in more detail.

### 12.3.1 Using a sparse checkout

A *sparse checkout* is a Git feature that allows you to check out only specific files or directories from a Git repository, rather than the entire repository. This can be useful in situations where you only need a portion of the files in a large repository, which can help save disk space and improve checkout and update times. The v4 GitHub action supports this command by specifying a `fetch-depth` of 0 as the default, and you can define which folders in the repo you want to get to your local disk. This can help prevent the download of, for example, all your documentation and large files while building your code or the reverse: only getting your documentation and not your code files that you don’t need when building your documentation. In the following listing, you can see how to configure a sparse checkout on the repository we use in our book, only to get the sources of the `frontend`, `ordering`, and `catalog` services.

**Listing 12.4 Sparse checkout**

```yml
- uses: actions/checkout@v4
  with:
    fetch-depth: 0
    sparse-checkout: |
      frontend
      ordering
         catalog
```

### 12.3.2 Adding caching

Workflow runs often reuse the same outputs or downloaded dependencies from one run to another. When you run this on your local machine, things will be much faster, since package and dependency management tools, such as NuGet, Maven, Gradle, npm, and Yarn, keep a local cache of downloaded dependencies. Because we get a new, fresh machine every time we run a job, we always have the hit of downloading all packages and dependencies from scratch. Overall, this incurs a longer wait time for the workflow to finish and can cause extra costs in network usage. To help speed up the time it takes to download files like dependencies, GitHub can cache files you frequently use in workflows.

You can use GitHub’s cache action to cache dependencies for a job. This action creates and manages a cache based on a unique key for each item you want to cache. You can also set up caching for the dependency managers by using their available setup actions. With those setup actions, setting up a package manager cache takes almost no effort. It is worth mentioning that the cache will take up disk space, which you must pay for. Table 12.1 shows the setup actions available for different package managers.

**Table 12.1 Setup actions available for package managers**

| **Package managers** | **setup-* action for caching** |
|----------------------|--------------------------------|
| npm, Yarn, pnpm      | setup-node                     |
| pip, pipenv, Poetry  | setup-python                   |
| Gradle, Maven        | setup-java                     |
| RubyGems             | setup-ruby                     |
| Go                   | setup-go                       |

If you set up a cache for npm, you can use the setup action that is part of the npm actions. In this case, you set it up as follows.

**Listing 12.5 Setting up npm cache**

```yml
- uses: actions/setup-node@v3
  with:
    node-version: 16
    cache: 'npm'
```

You can also set up a cache for NuGet, but in this case, you need to configure the cache action to take care of the files yourself.

**Listing 12.6 Setting up a NuGet Cache**

```yml
- uses: actions/cache@v3
  with:
    path: ~/.nuget/packages
    key: ${{ runner.os }}-nuget-${{ hashFiles('**/packages.lock.json') }}
    restore-keys: |
      ${{ runner.os }}-nuget-
```

As you can see, the cache that is set up uses the path on the runner that is `~/.nuget/packages`. This is the default location where NuGet caches its packages when running on your local machine. You now define this location as a location to get cached. Next, we set the cache key for this item to be unique for the `runner.os` and the contents of the packages.lock.json file.

> [!NOTE]
> 
> The `hashFiles(path)` function is available in specific expression contexts, and it will return a single hash value for the files that match the path patterns, separated with commas. This function calculates an individual SHA-256 hash for each matched file and then uses those hashes to calculate a final SHA-256 hash for the set of files. In our example, when the hash value is the same, the cache will be used; otherwise, we know something has changed and we cannot use the cache.

Another major source of savings can be the caching of container images. The Docker GitHub action can make use of the GitHub Actions cache. For this, you need to set the `cache-from` and `cache-to` properties on the action. It is also important to note when caching container images that your cache will need considerable storage. You also need to be aware of the cost implications of storing that data for a longer period. Storage costs are relatively low compared to other costs, but it is still something to be aware of and check regularly to ensure you are not wasting storage and incurring unwanted costs. You can check this on your organization’s Billing and Plans page, on which you can find how much you’ve spent (see figure 12.3).

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH12_F03_Kaufmann.png)<br>
**Figure 12.3 Storage cost for the organization**

You can set up container builds with caching by using the Docker action as follows.

**Listing 12.7 Setting up Docker to use the cache**

```yml
- name: Build and push
  uses: docker/build-push-action@v4
  with:
    context: .
    push: true
    tags: "<registry>/<image>:latest"
    cache-from: type=gha
    cache-to: type=gha,mode=max
```

After you have enabled the cache, you will see that the first time the cache option is used, the workflow run takes a bit longer, since it is building and saving the cache. On the next run, you see a significant reduction in time, since the action now uses all the cached layers during the build of the container image (see figure 12.4).

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH12_F04_Kaufmann.png)<br>
**Figure 12.4 The effects of caching**

### 12.3.3 Detecting a cache hit and skipping the work

When you cache your files that get created during the CI or CD process, you need to check if you need to produce the output or use it from the cache. For this, you can use the context information produced by the cache.

When you have a step where you retrieve data from the cache, you should give it a step ID. This ID can be used to get information about the step and to see whether it got the data from the cache. The following listing shows an example of one step pulling data from the cache and then the next step deciding if it needs to execute, using the `if()` statement as part of the step. You can never assume the cache will provide any data, so make sure your workflow is not dependent on having data in the cache.

**Listing 12.8 Caching your own files**

```yml
    - name: Generate file
      id: cache-file
      uses: actions/cache@v3
      with:
        path: file-location
        key: ${{ runner.os }}-file
  
    - name: Generate large file
      if: steps.cache-file.outputs.cache-hit != 'true'
      run: /generate-file.sh -d file-location
    - name: Use large file
      run: /myscript.sh -d file-location
```

### 12.3.4 Selecting other runners

By default, you can use the hosted runners that GitHub manages, which run on standard hardware. GitHub hosts Linux and Windows runners on standard machines with the GitHub Actions runner application installed. A few options can help you speed up your builds or reduce your costs if you run a lot of builds.

#### Speeding up builds with larger runners

If you run workflows that need more horsepower than the standard provided hosted runners, you can enable the use of hosted larger runners at the GitHub Enterprise level. These hosted larger runners are placed in a runner group, which becomes available as a target to run your workflows on. This is done by adding the group statement to the runs-on part of the workflow YAML file. In the following listing, you can see an example of running your workflow on a large hosted runner in the runner group `contoso-runners`.

**Listing 12.9 Running on large runner group provided by your enterprise**

```yml
jobs:
  build:
    runs-on: 
      group: contoso-runners
    steps:
    - uses: actions/checkout@v4
```

We discussed large hosted runners already in chapter 5, but it is important to note they can help you significantly reduce the time of your workflow execution. This can even save you costs, but that is a matter of experimentation. You save costs because the workflows can run faster, while the minutes themselves cost more—hence the need for some experimentation on this.

#### Lowering your costs with self-hosted runners

When you want to fully control the hardware the workflows run on, you can also use your own runners. When using self-hosted runners, you run an agent on a piece of hardware you own yourself. Self-hosted runners offer more control of hardware, operating system, and software tools than GitHub-hosted runners provide. With self-hosted runners, you can create custom hardware configurations that meet your needs, with the processing power or memory to run larger jobs, install software available on your local network, and choose an operating system not offered by GitHub-hosted runners.

You are not charged for self-hosted runners, so you can run infinite minutes on your hardware. The downside is that you need to manage all of this yourself, including the security hardening and patching. All the details of setting up your self-hosted runners are covered in chapter 6.

## 12.4 Optimizing your jobs

One final recommendation is to analyze your job runs and look at the time they run, how much is run in parallel, and how long jobs take. For example, instead of running everything in parallel in a situation where each job consumes less than a minute of time, it might be more cost effective to run this set in sequence instead, since you always pay per minute, rounded up to at least 1 minute. For example, if you run 10 jobs in 30 seconds, running them in one sequence in one job can save you 5 minutes of billing.

Also, ensure you don’t run actions that are not useful anymore because of previous outcomes of other steps or jobs. If a unit test fails, it is probably not very useful to lint your code or run further security checks, since the change needs to be fixed before you can continue. Hence, splitting the workflows by each goal you want to achieve, as described in chapter 8, is a best practice to prevent this from happening.

## Summary

* You can use concurrency groups if you have high-volume builds that take longer to complete. This way, you cancel the workflows mid-flight, saving you action minutes.

* You can use merge queues to optimize workflows for a high volume of committers on a branch.

* You can improve the performance of workflows by using sparse checkouts, caching, and using large runners.

* Keep an eye on your storage cost, and optimize your jobs to run efficiently in the minute spectrum, so you don’t waste money on action minutes and storage.