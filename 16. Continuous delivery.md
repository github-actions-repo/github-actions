- [Continuous delivery](#continuous-delivery)
  - [9.1 CD workflow steps](#91-cd-workflow-steps)
    - [9.1.1 Steps to deploy our GloboTicket application](#911-steps-to-deploy-our-globoticket-application)
    - [9.1.2 Triggering the deployment](#912-triggering-the-deployment)
    - [9.1.3 Getting the deployment artifacts](#913-getting-the-deployment-artifacts)
    - [9.1.4 Deployment](#914-deployment)
    - [9.1.5 Verifying the deployment](#915-verifying-the-deployment)
  - [9.2 Using environments](#92-using-environments)
    - [9.2.1 What is an environment?](#921-what-is-an-environment)
    - [9.2.2 Manual approval](#922-manual-approval)
    - [9.2.3 Environment variables](#923-environment-variables)
    - [9.2.4 Dealing with secrets](#924-dealing-with-secrets)
  - [9.3 Deployment strategies](#93-deployment-strategies)
    - [9.3.1 Deploying on premises](#931-deploying-on-premises)
    - [9.3.2 Deploying to cloud](#932-deploying-to-cloud)
    - [9.3.3 OpenID Connect (OIDC)](#933-openid-connect-oidc)
      - [Authentication action with OIDC for Azure](#authentication-action-with-oidc-for-azure)
      - [Authentication action with OIDC for Amazon Web Services](#authentication-action-with-oidc-for-amazon-web-services)
      - [Authentication action with OIDC for the Google Cloud Platform](#authentication-action-with-oidc-for-the-google-cloud-platform)
    - [9.3.4 Using health endpoints](#934-using-health-endpoints)
      - [Using metrics endpoints](#using-metrics-endpoints)
    - [9.3.5 Deployment vs. release](#935-deployment-vs-release)
      - [Feature toggles](#feature-toggles)
      - [Traffic routing](#traffic-routing)
    - [9.3.6 Zero-downtime deployments](#936-zero-downtime-deployments)
    - [9.3.7 Red–green deployments](#937-redgreen-deployments)
    - [9.3.8 Ring-based deployments](#938-ring-based-deployments)
      - [Canary release](#canary-release)
  - [Summary](#summary)

# Continuous delivery

This chapter covers

* Determining the basic steps of continuous delivery
* Deploying the sample application to production
* Using environments to guard deployments
* Implementing various deployment strategies
* Separating infrastructure and application code

*Continuous delivery* (CD) is a DevOps practice in which we deploy our software to production fully automated. In DevOps, we strive for a continuous flow of value to the end customer, which also means into production. The “holy grail” here is that every commit to the version control repository will end up in production in the shortest time possible with as few human interactions as possible, while delivering a stable, high-quality product.

## 9.1 CD workflow steps

The steps involved in moving to production vary greatly, depending on the product you build and the technologies you use. But, in general, you can state there are a set of generic steps you always want to take before users are exposed to new functionality. There are situations where everything is done in production, including testing the software. Although this is technically production, they keep the same safety measures in place as when you would go through a set of environments that are not exposed to the users. In general, the steps to move your software to production are as follows:

1. Deploy to an environment where you can test and validate the workings of the product.

2. Sign off on the product, based on the artifacts produced during verification.

3. Expose the software to the users the moment all quality checks have been executed and approvals have been given. These approvals can be manual, automated, or a combination of the two. GitHub provides many different options to help you move your software to production in a secure and compliant way.

In the following sections, we will further dive into how you can create your CD workflows, using the GloboTicket application to help us achieve this.

### 9.1.1 Steps to deploy our GloboTicket application

The GloboTicket application will go through a set of environments before it gets deployed to production. We will explore various options, including moving to a ring-based deployment strategy using environments.

The basic setup of the deployment is as follows:

1. *Get the deployment artifacts that we can use to configure Kubernetes to pick up the changed containers*. This entails downloading the prepared artifacts in the CI stage, as described in the previous chapter.

2. *Deploy to an internal staging server, where you can verify whether the software works as expected*. This goes beyond the tests that we have completed during the CI workflows. The tests we include here are validations that will check whether the software is deployed, the software is running in a healthy state, and the primary use cases of the software succeed. This is often achieved using end-to-end tests. In our sample application, we will use Playwright as the tool to complete end-to-end verification, but this can be done with many other tools as well. The most commonly used tools include Cypress, Selenium, Appium, and Playwright.

3. *Move the application to the next stage, often production*. It is up to you to determine whether this step involves moving to production and, if so, how the software will be moved to production. These days, we often need our software to run 24/7 without any downtime. For this, we can use all kinds of deployment patterns that enable deployments without downtime. This is described in section 9.3, where we cover several deployment strategies.

To summarize, the high-level steps involve deploying in a test environment, deploying in a staging environment, and then deploying to production. Now, the question is, *How should you determine when you are ready to move from one stage to the next?* For this, we can build in manual or automated approvals. We will show both types of approvals that we will use to deploy our sample application. Figure 9.1 shows the high-level stages we distinguished. In the following sections, we will examine each stage and share some common patterns and practices you can use to set them up.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH09_F01_Kaufmann.png)<br>
**Figure 9.1 High-level stages**

### 9.1.2 Triggering the deployment

The whole deployment process starts the moment a GitHub release is created. In our CI workflows, we create a release the moment we want those features to move to production and, preferably, every commit to the main branch of the repository. During development, the most common approach consists of developing features in feature branches and then creating a pull request the moment they are ready. This pull request will be verified with a CI workflow, and approving the pull request to be merged into the main branch is normally the trigger point for the whole CI/CD cycle to start.

The release created after all CI workflows have finished acts as the hand-off moment to trigger a release. In GitHub, there is an event for this, which we can use to trigger the CD workflow. The release itself will contain the artifacts we want to release. So part of the CI work was to produce the artifacts and make them available for the release. The following listing shows the beginning of the CD workflow with the trigger on the release.

**Listing 9.1 Starting the deployment**

```yml
on:
  release: 
     types: [published]
```

### 9.1.3 Getting the deployment artifacts

The release contains the artifacts we want to use for our deployment workflow. In our case, the application is deployed to a Kubernetes cluster, and we are using containers. When using containers, there is no need to get or download the container images, since they will be pulled from an image registry by the cluster. We do need the deployment files, created during the CI process, containing information on which containers use the new version of the software and should, therefore, be pulled from the registry.

To get the artifacts that are part of the release, you can use an action called `dsaltares/fetch-gh-release-asset`. This action has a set of options to retrieve the artifacts and save them, so they can be used in the next steps in the workflow. The action requires us to provide a version so that it knows which version of the available releases needs to be queried for the artifacts. We can get this information from the trigger of the workflow event. By creating an expression that retrieves the version number from the event, we can retrieve the artifacts from the release. This is simply the expression:

```yml
${{ github.event.release.id }}
```

Next, you can either provide the exact files you are looking for or provide it with a regular expression that defines which files we are interested in. In our case, this is the set of YML files we need to deploy to the cluster and define the application deployment on Kubernetes. We specify that we want to store the files in the current folder of the runner. The following listing shows how to configure the action to retrieve the necessary artifacts.

**Listing 9.2 Retrieving artifacts**

```yml
uses: dsaltares/fetch-gh-release-asset@1.1.1
with:
  version: ${{ github.event.release.id }}
  regex: true
  file: ".*"
  target: './'
```

Our next step is to use these files to complete the deployment.

### 9.1.4 Deployment

Where you want to deploy your app is, again, very specific to your organization or the software you created. To show how to deploy, we’ve chosen to deploy to Kubernetes, since this is a fast-growing ecosystem and is provided by many cloud providers. In our examples, we will deploy to a set of Kubernetes clusters that we run in the cloud. In the following sections, we will call out variations that you can use for other environments, but for this chapter, we will focus on our Kubernetes cluster hosted in the Azure cloud.

Deploying to a Kubernetes cluster requires you to use of a set of actions that help us interact with the cluster. The main way to interact with Kubernetes is via a command-line tool, called `kubectl`. You can choose to simply use the command-line tool, or you can use a set of actions that are available to interact with these tools, which will make the workflows a bit less difficult to read and to maintain. The set of actions we use for the interaction with Kubernetes are `azure/k8s-set-context`, `azure/k8s-create-secret`, and `azure/k8s-deploy`. There is nothing Azure specific about these actions; they are created by Microsoft and are found in the Azure action repository.

We start by setting the context, in which these actions’ “behind-the-scenes” command-line tools can connect to the cluster. This is done by using a file called kubeconfig, which you can find in your .kube folder on your system. This file contains the information needed to connect to the cluster. Since this is something we need to keep as a secret, we will reference the data of this file using the built-in secret variables feature of GitHub. We name this secret `KUBECONFIG`. The following listing shows the steps to deploy to the Kubernetes cluster.

**Listing 9.3 Deploying to Kubernetes**

```yml
- name: set kubernetes context
  uses: azure/k8s-set-context@v3
  id: setcontext
  with:
    method: kubeconfig
    kubeconfig: ${{secrets.KUBECONFIG}} 
  
- name: provide pull secrets so we can pull the image from gitHub
  uses: azure/k8s-create-secret@v4
  with:
    namespace: '${{env.namespace}}'
    secret-name: 'pullsecret'
    container-registry-url: 'ghcr.io'
    container-registry-username: ${{ github.actor }}
    container-registry-password: ${{ secrets.EXTENDED_ACCESSTOKEN }}
  
- name: Deploy to AKS
  uses: Azure/k8s-deploy@v4
  with:
    namespace: '${{env.namespace}}'
    manifests: |
       ./${{env.deploymentFile}}
```

For certain things, like namespaces, you see the expression syntax is used to read a variable from the environment. In this example, the namespace we use is `globoticket`, which you specify in the `env:` section at the top of the workflow. This makes it a bit easier to maintain. Common practice here is to put information in an environment variable the moment you need to repeat yourself, since you know that might be something you’ll need to change in the future. This way, there will only be one place you need to make the change, instead of having several scattered throught the workflow file.

### 9.1.5 Verifying the deployment

To begin verification, we start a new job. This job can run the tests we want to run to validate whether our deployment was successful. In our sample application, we created a simple end-to-end test, where we click through the application, using Playwright as our tool of choice. We also need the artifacts from the release, in this case. Conveniently, we already have them on our system, based on the first step in our action workflow. So the only steps we need to take are to call the test tool, run it, and provide it the correct endpoint to find the web application we just deployed. The steps to test our deployment are shown in the following listing.

**Listing 9.4 Steps to test our deployment**

```yml
    - name: Install playwright 
        run:  dotnet tool install --global Microsoft.Playwright.CLI
   
      - name: set homepage from deployment
        run: |
          export homepage=$(kubectl get svc frontend --namespace ${{env.namespace}}-o jsonpath='{.status.loadBalancer.ingress[0].ip}')
          echo "homepage=$homepage" >> $GITHUB_ENV
   
      - name: Run tests
        run: |
          dotnet test Tests.Playwright/Tests.Playwright.csproj --logger "liquid.md;logfilename=testResults.md"
      - name: Output the results to the actions jobsummary
        if: always()
        run: cat $(find . -name testResults.md) >> $GITHUB_STEP_SUMMARY
```

After we install Playwright, we run the tests that show the results of the deployment and report this back as part of the completed workflow of this job.

Note that in the action workflow, we use the option `if: always()`, which ensures that the test results are always added to the step summary. If we were to leave this to the defaults, no report would be added the moment any of the tests failed, since the previous step would produce an error and the workflow would be aborted.

This is the report that can be used to decide on a manual approval, and it can also be used for compliance, which shows that the application was verified before being taken into production. You can also choose to fail the job on a failed test, which will stop further jobs that express a dependency on this job. It is possible to, for example, take screenshots during the execution and make them part of the results as well.

## 9.2 Using environments

GitHub introduced the concept of environments to accommodate for the fact that most organizations traditionally use real physical different environments to test software before it moves to production. This is a common practice in our industry, and with each transition between those physical environments also comes a set of rules or constraints as part of the processes to move software to production. Although we strive to minimize all kinds of hand-overs in the deployment process when we implement DevOps, this does not mean these processes will be gone over night.

Environments in GitHub allow us to map the environments we deploy to, to the process we want to follow, while automating as much as we can. Hence, environments provide us with capabilities that can help us further automate the deployments, while embracing compliance and existing processes in organizations.

### 9.2.1 What is an environment?

From an action workflow perspective, an *environment* is available by adding a reference to the environment in our workflows. This reference is done as part of the job definition, which means you tie a job to an environment. If you want to use multiple environments, this also implies you need multiple jobs.

Note that when you define an environment that does not yet exist, it will be automatically created. You can choose to create them up front, but this is not required. The following listing shows how to reference an environment in your workflows at the job description level.

**Listing 9.5 Referencing an environment**

```yml
  deploy:
    runs-on: ubuntu-latest
    needs: build
    environment:
      name: 'staging'
      url: ${{ steps.deploy-to-webapp.outputs.webapp-url }}
```

In this listing, we have a job with the name `deploy`, and we reference the environment with the name `staging`.

You also see the environment has a URL. This URL will be shown in the reports and the diagrams in the workflow visualization on the website (see figure 9.2). This is very handy, since it is the simplest way to provide access to the deployed application. You see in the example that the URL is set by referring to the step in the job with the name `deploy-to-webapp`, which has an output variable `webapp-url`. The moment the step is executed in the job and has a value, it will be shown.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH09_F02_Kaufmann.png)<br>
**Figure 9.2 The environment URL**

### 9.2.2 Manual approval

After deploying and verifying, the next step is to decide whether you are ready to begin deployment in production. This decision point is also available when using environments, since you can configure an environment to require approval. You can set multiple approvers that need to manually approve the entry of the environment by the workflow. This means the job to execute the deployment is not started before this approval is given. The moment the approval is given, this is registered in the system, so you get full traceability of the deployment and the approvals. This is especially important for companies that operate in highly governed industries, like healthcare, pharmaceuticals, and finance. You can define the approvals in the configuration of the environment, as shown in figure 9.3.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH09_F03_Kaufmann.png)<br>
**Figure 9.3 Configuring approvals**

You can define who the approver of the environment is, and you can also define if you want a job to wait for a certain period of time before it is started. This can be helpful if you want a staggered release where you, for example, want to slowly ramp up traffic to the application. We will get back to this in section 9.3.8.

### 9.2.3 Environment variables

When you run a job, you will get a default set with environment variables you can use. These environment variables can be set as part of your workflow script or as the trigger of the workflow script. These environment variables can also be overridden in an environment. This provides the option to reuse a deployment script in multiple environments, and by changing only the values of the variables, you can change the place where the deployment takes place.

In our scenario, we can deploy first to staging, using the exact same workflow, and provide the environment variables for each environment. Figure 9.3 from the previous section shows how you can set the value of the environment variable for the environment. Using the exact same name will override the repository variable.

Working with environment variables enables you to create fully reusable workflows and ensure each environment gets deployed in the exact same way. This is generally the best practice, since it reduces the variability of the various environments, ensuring fewer problems will occur in the final production deployment.

### 9.2.4 Dealing with secrets

When we need to log into a server when we want to deploy, in our case, to the Kubernetes cluster, we need to have credentials. Credentials are dangerous to spread in your source code or in deployment scripts. To avoid putting these credentials in your scripts, we can use GitHub secrets. Secrets can be set in the Configuration Settings page under Secrets for Actions (see figure 9.4).

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH09_F04_Kaufmann.png)<br>
**Figure 9.4 Setting secrets**

Secrets are safely stored inside GitHub, and you can only change them via the portal; you cannot read them or list them from the portal, for obvious reasons. When we need to use secrets in our workflows, we reference the secret using an expression. If we want to get the secret value stored in GitHub, we can use the following syntax: `${{secrets.NAMEOFSECRET }}`.

Environment secrets

Environments also provide a way to override the secret at the environment level. So by referencing an environment for your job, you switch the context from which secrets are retrieved. You can set the secrets for each environment the same way you define a standard secret. The syntax in the workflow stays precisely the same.

## 9.3 Deployment strategies

When it comes to deploying your application, you can use multiple strategies that have been discovered over the years by many different vendors. All strategies require us to separate the actual deployment of the software and the reveal of the new software. This means we need to separate deployment from release. Let us elaborate more on this before diving into some very common strategies.

### 9.3.1 Deploying on premises

You often need to deploy to (virtual) machines running in your own data center. When this is the case and hosted GitHub runners cannot access those machines, you need to set up your own set of runners that will run in your private data center. In chapter 6, we described how to install these runners. Once the runners are available, you can designate your job definition to use the self-hosted runners to execute the workflow for your on-premises deployments. When running a job on a private runner, you can still use the concept of an environment and have the same options, like manual approvals, delayed execution of the job, and the traceability of the deployment tied back to the commit in source control.

### 9.3.2 Deploying to cloud

When we deploy to a cloud environment, we often can use the hosted runners provided by GitHub, although this may depend a bit on the network setup chosen in the cloud provider. Your hosted runner needs access to the (virtual) machines or the platform as a service infrastructure on which you have chosen to run your application.

Before the workflow can access our cloud resource, it needs to supply credentials, such as a password or token, to the cloud provider. These credentials are usually stored as a secret, as described in the previous paragraphs. This way, the workflow can present the required secret to the cloud provider every time it runs.

However, using secrets this way requires you to create credentials in the cloud provider and then duplicate them in GitHub as a secret. This is a maintenance hassle and has the additional risk of disrupting deployments because secrets expired, got rotated, and so on. There is an alternative, called OpenID Connect, which takes a different approach.

### 9.3.3 OpenID Connect (OIDC)

With *OpenID Connect* (OIDC), we configure a workflow to request a shortlived access token directly from the cloud provider. This is done by setting up a trust relationship between the cloud provider and the GitHub repository where the workflow runs. The moment the workflow executes, it will authenticate with the execution context, like the name of the org and repo or the name of the workflow and branch, and then use the credentials obtained in the OIDC handshake to execute the deployment.

Setting up OIDC is a bit involved for the initial trust relationship setup. After this is done, you enable the most secure way of authenticating your workflows with your cloud provider for deployments.

To use OpenID Connect, your cloud provider needs to support it on their end. Providers that currently support OIDC include Amazon Web Services, Azure, Google Cloud Platform, and HashiCorp Vault, among others.

There are three steps to enable OIDC in your workflow, with the last two steps involving making some changes in your YAML:

* Create the trust relationship between your cloud provider and the GitHub repository.
* Add permissions settings for the token.
* Preferably, using the official action from your cloud provider, exchange the OIDC token (JWT) for a cloud access token.

The change in permissions is needed to complete the OIDC handshake. For this to succeed, you need to add permissions at the workflow level or the job level and give it write rights to the `id-token`:

```yml
permissions:
   id-token: write
```

Setting up the trust relationship between the cloud provider and GitHub is beyond the scope of the book. You can read the details on setting up the trust for your cloud provider here on [GitHub][1].

> [!WARNING]
> 
> During set up and configuration of the OIDC trust relationships, ensure only the appropriate org or repo can deploy to a specific set of resources. Make sure you don’t accidentally configure *any* GitHub repo (globally) to be able to deploy into your cloud resources. Such a broad scope can cause problems that you want to minimize.

#### Authentication action with OIDC for Azure

If you are using Azure as your cloud provider, the action you can use to authenticate with Azure after setting up the trust relationship is shown in the following listing.

**Listing 9.6 OIDC for Azure**

```yml
name: Run Azure Login with OpenID Connect
on: [push]
  
permissions:
      id-token: write
      contents: read
      
jobs: 
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
    - name: 'Az CLI login'
      uses: azure/login@v2
      with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
   
    - name: 'Run Azure CLI commands'
      run: |
          az account show
          az group list
          pwd
```

As you can see, there are three secrets used in this workflow example. The secrets are not actual credentials; they are identifiers of the subscription, the tenant, and the service principal used in the trust relationship. These secrets are things that are, by themselves, not enough to authenticate—the trust relationship between the GitHub repository and your Azure subscription is what makes the authentication work.

Besides using the Azure CLI, you can also use all other Azure actions that encapsulate the deployment to specific resources in Azure. For example, deploying to the Kubernetes cluster is done using the `azure/k8s-set-context`, `azure/k8s-create-secret` and `Azure/k8s-deploy` actions, which all understand the authentication handshake performed in the previous action.

#### Authentication action with OIDC for Amazon Web Services

If you are using Amazon Web Services (AWS) as your cloud provider, the action you can use to authenticate with AWS after setting up the trust relationship is shown in the following listing.

**Listing 9.7 OIDC for AWS**

```yml
name: AWS example workflow
on:
  push
env:
  BUCKET_NAME : "<example-bucket-name>"
  AWS_REGION : "<example-aws-region>"
# Permission can be added at the job or workflow level.   
permissions:
      id-token: write   # This is required for requesting the JWT.
      contents: read    # This is required for actions/checkout.
jobs:
  S3PackageUpload:
    runs-on: ubuntu-latest
    steps:
      - name: Git clone the repository
        uses: actions/checkout@v4
      - name: configure aws credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::1234567890:role/example-role
          role-session-name: samplerolesession
          aws-region: ${{ env.AWS_REGION }}
      # Uploads a file to AWS s3
      - name:  Copy index.html to s3
        run: |
          aws s3 cp ./index.html s3://${{ env.BUCKET_NAME }}/
```

The `aws-actions/configure-aws-credentials` action will perform an `Assume-RoleWithWebIdentity` call and return temporary security credentials for use by other actions. This action implements the AWS SDK credential resolution chain and exports environment variables for your other actions to use. Environment variable exports are detected by both the AWS SDKs and the AWS CLI for AWS API calls. For example, deploying to an ACS cluster would involve using the following actions: `aws-actions/amazon-ecr-login`, `aws-actions/amazon-ecs-render-task-definition`, and `aws-actions/amazon-ecs-deploy-task-definition`. All these actions understand the credentials saved in the environment via the `configure-aws-credentials` action.

#### Authentication action with OIDC for the Google Cloud Platform

If you are using the Google Cloud Platform (GCP) as your cloud provider, the action you can use to authenticate with GCP after setting up the trust relationship is shown in the following listing.

**Listing 9.8 OIDC for GCP**

```yml
name: List services in GCP
on:
  pull_request:
    branches:
      - main
  
permissions:
  id-token: write
  
jobs:
  Get_OIDC_ID_token:
    runs-on: ubuntu-latest
    steps:
    - id: 'auth'
      name: 'Authenticate to GCP'
      uses: 'google-github-actions/auth@v2'
      with:
          project_id: 'my-project'
          workload_identity_provider: '<example-workload-identity-provider>'
  
    - id: 'gcloud'
      name: 'gcloud'
      run: |-
        gcloud auth login --brief --cred-file="${{ steps.auth.outputs.credentials_file_path }}"
        gcloud services list
```

The preferred way to authenticate with GCP is by using the Direct Workload Identity Federation. This is preferred, since it directly authenticates GitHub Actions to Google Cloud without a proxy resource. However, not all Google Cloud resources support `principalSet` identities at the moment of writing.

The `google-github-actions/auth action` receives a JWT from the GitHub OIDC provider and then requests an access token from GCP. You can use, for example, the `google-github-actions/get-gke-credentials` action to retrieve the Kubernetes credentials to run a deployment using the kubectl command-line tools.

When environments are used in workflows or OIDC policies, it is recommended to add protection rules to the environment for additional security. For example, you can configure deployment rules on an environment to restrict which branches and tags can deploy to the environment or access environment secrets.

### 9.3.4 Using health endpoints

One way to protect your deployment from disrupting your application’s current users is by using health endpoints your application can provide. These health endpoints are usually available as part of your application and will return the health status of your application. Typically, you will have the endpoints `/health/ready` and `/health/lively`, which can be called to provide a JSON string with information about the application’s health. These endpoints were introduced when the industry started using Kubernetes, but they are also universally usable when you are not deploying to a Kubernetes cluster. The `/health/ready` endpoint normally signals if the application is ready to receive traffic. It either signals ok or not ok. This endpoint is normally probed multiple times during application start up. The moment it signals it’s ready, the deployment can move to the next stage. This can, for example, be the run of an end-to-end test, to see if the application functions as expected.

The `/health/lively` endpoint is typically used during normal operation of the application, and it can also signal the application is in a degraded or unhealthy state. This information can be used in more advanced scenarios, where you move traffic slowly to the new deployment and then closely monitor if the application stays healthy. When you detect a degraded or unhealthy state for a couple of requests, you can decide to abort the deployment and roll back to the last known good state.

You can use some simple script to wait for the application to become healthy after installation. This assumes the `ready` endpoint will eventually return `healthy` in the JSON response. This is shown in the following listing.

**Listing 9.9 Checking the health status**

```yml
      - name: ensure deployment is healthy before we test
        run: |
            HEALTH_ENDPOINT="https://${{stagingurl}}/health/ready"
            while true; do
              response=$(curl -s "$HEALTH_ENDPOINT")
              status=$(echo "$response" | jq -r '.status')
  
              if [[ "$status" == "Healthy" ]]; then
                echo "Health endpoint current status :" $status
                break
              fi
              echo "Waiting for health endpoint..."
              sleep 1
            done
        env: 
          homepage: ${{ needs.deploy.outputs.homepage }}
```

#### Using metrics endpoints

Metrics endpoints can deliver metrics that can be monitored over time. A metrics endpoint normally produces exactly the same amount of data, and this can be captured over time. One data point a metric endpoint can deliver is the amount of memory an application uses. When you are running a deployment where you know the memory usage profile of the current deployment, you can use that as a known good state. By monitoring the new deployment and comparing it against the known good profile, you can detect problems early in the deployment process and use this information to determine if you want to proceed or revert to the last known good state. This is also used in more advanced scenarios and is often combined with tools like DataDog or Azure Monitor that have built-in AI capabilities to detect anomalies and, based on that signal, problems you can pick up. These allow you to abort your deployment, or at least apply additional manual intervention, before you continue.

### 9.3.5 Deployment vs. release

Traditionally, when we deploy our software to a server, we often release the new capabilities to our users in one step. To support higher deployment frequencies and strategies that enable software deployment scenarios that don’t disrupt normal operations from an end user’s perspective, we need to separate the two concerns of deployment and release.

Deploying software implies installing the artifacts you created on the servers hosting the software for the end users. This installation can be done without exposing the new features or the changes that are part of that software release. For this, we can use various techniques, of which the most commonly used include feature toggles and traffic routing. You will learn later that combining these two techniques is also possible and enables more-advanced scenarios.

#### Feature toggles

When you use *feature toggles*, you place the changed software behind a switch that you can influence separately from the deployment of the software. In its most basic form, this is an if statement that shows the old code in action when not activated and the new code the moment the feature toggle is changed to reveal the new implementation. This enables you to install the software on the servers, expose it to the end users, and still show the old version of the software. The next step is to reveal the new version of the software at a later moment.

This enables you to first install the software and then validate if it is still working in production as it was with the old version. Then, you can see the effects of the new version of the software being used. This can provide additional insights into the quality of the new version and gives a very simple way to roll back the deployment; this is now nothing more than flipping the switch back to off again, showing the old version. This also enables ring-based deployments or gradual exposure of a new version of the application.

#### Traffic routing

With *traffic routing*, you install the software on new servers or servers that don’t have any traffic flowing into the server. After installing the software, you gradually start routing traffic to the new version of the software. You can do this in steps with a percentage of the traffic that is flowing into the servers, or you can target a specific group of users with, for example, special headers that will be routed to the new version or variations in the query string if it is a web application. Many techniques can be used to determine which traffic you want to route to the new version, and that is part of the release strategy you choose. Also, in this case, you can determine how well the new software is doing and decide to roll back when you see abnormal behavior by simply routing the traffic to the existing well-known working version of the software.

### 9.3.6 Zero-downtime deployments

A *zero-downtime deployment* is the deployment of the new version of the software without interrupting the operation of the software. As stated in section 9.3.1, we use feature toggles and traffic routing to accomplish this goal. Another crucial point is that the application is stateless, meaning it does not keep state in memory of the application across multiple invocations. It also needs to be able to deal with multiple versions of the application running at the same moment. This is because your application will be active on both versions during the gradual reveal of the new deployment. The application needs to be built so that it can handle these situations.

So zero downtime is not for every application and depends on the type of software you are building. Our sample application can be deployed like this, since it is designed to be stateless, making it possible to swap out a container without interrupting the users, assuming existing requests will be served to completion before the container instance gets removed from the cluster. Luckily, this is one of the features of a Kubernetes cluster, and the Kubernetes environment will take care of this.

We could also deploy our application to a web server farm. In this case, we need to take care of deploying the software to a server that is not taking any traffic yet, deploy the software, and then move traffic to the server for new requests. This way, we also enable the deployment without any downtime.

### 9.3.7 Red–green deployments

A *red–green deployment* is a way to deploy an application without any visible downtime for the end user. It is commonly used when you deploy a web application to production. In this case, you don’t want to interrupt the active and you also don’t want any downtime for the web application. In this situation, you set up an infrastructure that has at least two web servers and a network load balancer that you can use to control the traffic. The basic setup is shown in figure 9.5.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH09_F05_Kaufmann.png)<br>
**Figure 9.5 Environment setup**

We assume we have the web application running on one server, which is in the production environment. Now, we want to start a new deployment. We deploy the software to the other server (the red server in the staging environment), and we have all traffic routed to the green server in the production environment. After we deploy the software to the red server, we validate it is all up and running and performing as expected. Next, we start moving a little bit of traffic to the new deployment. While we do this, we closely monitor the application using the application telemetry. After we have seen a small amount of traffic go via the new red server and don’t experience any problems, we can move more traffic to the new deployment. This is achieved via a set of steps—the number of which is up to you to decide. Finally, when we have moved all the traffic to the red server, this is what is shown as the virtual IP swap. From that moment on, the red server will be in the production environment and the green server in the staging environment. Next, we can start installing a new version on the green server again and repeat this same process. With this mechanism, you move the traffic from one version of your software to the other without any downtime, which enables you to release your software at any moment of the day.

In listing 9.10, you can find the deployment of our GloboTicket application as a web application being deployed to an Azure web application, using deployment slots (this is the secondary server where we install the software). In this sample, I am using the Azure command line to change the traffic that is routed to the newly deployed version of the application, and we use the health endpoints to verify the installation is still healthy, so we can progress our deployment.

**Listing 9.10 Red–green deployment**

```yml
name: Deployment globoticket Frontend
env:
  appname: globoticket
  resourcegroup: globoticket
  slotname: staging
on:
  release: 
     types: [published] 
We start by deploying the application to Azure Web App. We pull the zip file artifact that contains the web application from the release and use it directly to deploy to the web app. After deployment, we generate output, which is the URL we can use to validate the deployment:

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment:
      name: 'staging'
      url: ${{ steps.deploy-to-webapp.outputs.webapp-url }}
    outputs:
      homepage: ${{ steps.deploy-to-webapp.outputs.webapp-url }}
    steps:
      - name: get release artifacts for deployment
        uses: dsaltares/fetch-gh-release-asset@master
        with:
          version: ${{ github.event.release.id }}
          regex: true
          file: ".*"
          target: './'
  
      - name: Azure Login
        uses: azure/login@v2
        with:
          creds: ${{ secrets.AZURE_DEPLOY }}
        
      - name: Create Deployment Slot Staging
        uses: Azure/cli@v2.70.0
        with:
          inlineScript: |
            az webapp deployment slot create --name ${{env.appname}} --resource-group ${{env.resourcegroup}} --slot ${{env.slotname}}
          
      - name: Deploy to Azure Web App
        id: deploy-to-webapp
        uses: azure/webapps-deploy@v2
        with:
          app-name: '${{env.appname}}'
          slot-name: '${{env.slotname}}'
          package: ./frontend.zip
```

We use the output from the deploy job to validate the deployment. This contains the URL we can feed into our playwright tests that will validate the deployment. The tests will generate a report that is output to the GitHub Actions UI. This way, we can simply see the output results as part of the action run:

```yml
  validate:
    runs-on: ubuntu-latest
    needs: deploy
    environment:
      name: 'staging'
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
      - name: print env
        run: echo $homepage
        env: 
          homepage: ${{ needs.deploy.outputs.homepage }}        
      - name: Install playwright
        run:  npm init playwright@latest
          
      - name: Set up .NET Core
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: 8.0.x   
      
      - name: Install playwright 
        run:  dotnet tool install --global Microsoft.Playwright.CLI
```

Before we start our playwright tests, we use the available health endpoint on the application to check if the application is healthy. We do this by polling the health endpoint until it returns a status that signals healthy. This way, we can ensure the application is ready to be tested:

```yml
      - name: ensure deployment is healthy before we test
        run: |
            response=$(curl -s "${homepage}/health/ready")
            status=$(echo "$response" | jq -r '.status')
   
            while [[ "$status" != "Healthy" ]]; do
              echo "Waiting for health endpoint..."
              sleep 1
              response=$(curl -s "${homepage}/health/ready")
              status=$(echo "$response" | jq -r '.status')
            done
            echo "Health endpoint current status :" $status
        env: 
          homepage: ${{ needs.deploy.outputs.homepage }}
      - name: Run tests
        run: dotnet test Tests.Playwright/Tests.Playwright.csproj
        env: 
          homepage: ${{ needs.deploy.outputs.homepage }}
```

Here, we move to the next stage, where we will accept 10% traffic to the staging slot. The environment provides a way to move gradually to higher percentages of traffic. We do this by setting the environment wait timer in the GitHub UI to 1 minute. This way, we can see the traffic move gradually. In the meantime, you can monitor the application behavior and decide to abort when needed:

```yml
  staging10:
    runs-on: ubuntu-latest
    needs: validate
    environment:
      name: 'staging10'
    steps:
    - name: Azure Login
      uses: azure/login@v2
      with:
        creds: ${{ secrets.AZURE_DEPLOY }}
    - name: TenPercent
      uses: Azure/cli@v2.70.0
      with:
        inlineScript: |
          az webapp traffic-routing set --distribution ${{env.slotname}}=10  --name ${{env.appname}} --resource-group ${{env.resourcegroup}}
 #The same as the previous step, but now, trafic percentage increases to 30%.
  staging30:
    runs-on: ubuntu-latest
    needs: staging10
    environment:
      name: 'staging30'
    steps:
    - name: Azure Login
      uses: azure/login@v2
      with:
        creds: ${{ secrets.AZURE_DEPLOY }}
    - name: TenPercent
      uses: Azure/cli@v2.70.0
      with:
        inlineScript: |
          az webapp traffic-routing set --distribution ${{env.slotname}}=30  --name ${{env.appname}} --resource-group ${{env.resourcegroup}}
```

We are now confident the application behaves as expected, it can handle real production traffic, and we are ready to move the application to production for 100 percent. We do this by swapping the staging slot with the production slot:

```yml
  VipSwap:
    runs-on: ubuntu-latest
    needs: staging30
    steps:
      - name: Azure Login
        uses: azure/login@v2
        with:
          creds: ${{ secrets.AZURE_DEPLOY }}
      - name: vip swap
        uses: Azure/cli@v2.70.0
        with:
          inlineScript: |
            az webapp deployment slot swap --slot ${{env.slotname}}  --name ${{env.appname}} --resource-group ${{env.resourcegroup}}
  
      - name: clear Routing rules (100% to production)
        uses: Azure/cli@v2.70.0
        with:
          inlineScript: |
            az webapp traffic-routing clear --name ${{env.appname}} --resource-group ${{env.resourcegroup}}
```

We are now done with the staging slot, so we can delete it. This will also clear any routing rules we set, so we can start from a clean slate for the next deployment:

```yml
      - name: clear staging slot
        uses: Azure/cli@v2.70.0
        with:
          inlineScript: |
           az webapp deployment slot delete --name ${{env.appname}} --resource-group ${{env.resourcegroup}} --slot ${{env.slotname}}
```

### 9.3.8 Ring-based deployments

With a *ring-based deployment* you define different groups of users of your application. For each group, you determine the risk they can tolerate if you fail in your deployment or introduce an incident. Next, you set up your deployment to a set of environments that are all production, but the first set of users you expose to the new features are considered users in the first ring of exposure. You can use various deployment techniques, like a canary release.

#### Canary release

A canary release is a technique where you expose the new software to only a very select few users and ensure you have proper monitoring set up to determine whether the software and new functionality are behaving as expected. The moment you see anomalies, you can turn the feature off again, so you can guarantee normal operations. This is done by using either network traffic management or feature toggles. The name canary release refers to the canaries mine workers historically used to signal the presence of toxic gases in the mine shaft. If the canary died suddenly, miners knew they should exit the mine as quickly as possible because it was likely there were high levels of dangerous gases in the mine. This term has generally become an industry term for releasing software with feature toggles or traffic management to gradually expose new functionality and validate the behavior in production.

When you don’t see any anomalies in operations, often a few hours after deployment, you can determine if you want to move forward and expose the next group of users to the new software. The concept is shown in figure 9.6.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH09_F06_Kaufmann.png)<br>
**Figure 9.6 Ring-based deployment**

You can set this up by using multiple target environments that guard the ring. You need a mechanism to route traffic to a specific ring. This is part of your application design, and you can use various techniques for this. The simplest one is to have specific domain prefixes for every ring. You can map the users to a domain and use traffic routing to route the traffic to the correct ring. You only accept the progression of a deployment if there are no life site incidents in a particular ring after deployment and your telemetry shows the application is operating according to expected behavior.

The way this is orchestrated with a deployment workflow is very similar to the previous red–green example, but now, we will use environments that monitor multiple parameters, like the number of incidents and the metrics of the application in the workflow. The other capability we use is to wait for a certain amount of time. You can configure this in minutes, with a maximum of 43,200 minutes, or 30 days. This is shown in figure 9.7.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH09_F07_Kaufmann.png)<br>
**Figure 9.7 Setting the environment wait timer**

This is the amount of time you wait for the execution of the workflow tied to the environment that can check the aforementioned metrics. You can let the workflow endpoint fail the moment it determines there is an anomaly. This blocks the application from progressing to the next ring, and then you can restart that failing workflow when this problem is resolved.

## Summary

* We have a separate workflow file to handle the continuous delivery workflow because it provides the best way to separate the two goals we are trying to achieve with CI and CD.

* We use the release created in the CI workflow and pick all artifacts from there to get a consistent set of files we use for our deployment that is also versioned and traceable back to the changes in version control.

* We use the creation of a release as the trigger of the CD workflow, and we use the version numbers from the event to retrieve the production artifacts from the release.

* You can use tokens or Open ID Connect to deploy to your cloud environments, where Open ID Connect is the safest solution.

* A good practice is to use health endpoints to validate your deployment before you move to the next step in your deployment workflow and use environments to gate the next step in the deployment.

* There are various deployment strategies, including red–green deployments to deploy the GloboTicket application.

[1]: https://docs.github.com/en/actions/security-for-github-actions/security-hardening-your-deployments/about-security-hardening-with-openid-connect#enabling-openid-connect-for-your-cloud-provider