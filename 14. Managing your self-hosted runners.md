- [Managing your self-hosted runners](#managing-your-self-hosted-runners)
  - [7.1 Runner groups](#71-runner-groups)
    - [7.1.1 Assigning a runner to a runner group](#711-assigning-a-runner-to-a-runner-group)
  - [7.2 Monitoring your runners](#72-monitoring-your-runners)
    - [7.2.1 What to monitor](#721-what-to-monitor)
    - [7.2.2 Monitoring available runners using GitHub Actions](#722-monitoring-available-runners-using-github-actions)
    - [7.2.3 Building a custom solution](#723-building-a-custom-solution)
    - [7.2.4 Using a monitoring solution](#724-using-a-monitoring-solution)
  - [7.3 Runner utilization and capacity needs](#73-runner-utilization-and-capacity-needs)
  - [7.4 Monitoring network access](#74-monitoring-network-access)
    - [7.4.1 Monitor and limit network access](#741-monitor-and-limit-network-access)
    - [7.4.2 Recommended setup](#742-recommended-setup)
  - [7.5 Internal billing for action usage](#75-internal-billing-for-action-usage)
  - [Summary](#summary)

# Managing your self-hosted runners

This chapter covers

* Managing runner groups
* Monitoring your runners
* Finding runner utilization and capacity needs
* Internal billing for action usage

When you start creating your self-hosted runners, you will need to find out how and when your runners are being utilized, by which repositories and teams. With that information you can then both scale the runners appropriately and guide your users into better patterns of using them. There are options to segment runners into groups and only allow a group to be used by specific repositories (e.g., by a single team).

## 7.1 Runner groups

With runner groups, you can segment your runners into different clusters and manage access to the runners in the group with specific options. You can use runner groups, for example, to segment the runners for the repos of a specific team and make sure they always have a specific number of runners available. Or you can use them to make sure a group of runners with a certain capability (e.g., GPU-enabled runners) are only available to certain repositories and, thus, users. You do not want to run simple linting jobs on those expensive runners, so you better make sure to separate these runners from the default runners that have the `self-hosted` label!

Runner groups can only be created at the enterprise or organization level, not at the repository level. When you navigate in the organization to **Settings > Actions > Runner Groups**, you’ll find the overview of all your runner groups, as shown in figure 7.1. On the enterprise level, you can find runner groups under **Settings > Policies > Actions** and then clicking the **Runner Groups** tab. By design, there is always a group called `default`, where new runners get registered unless you indicate otherwise in the configuration process. New groups can only be created using either the user interface or by using the REST API, as shown in the following listing.

Listing 7.1 Creating a new runner group

curl -L \
  -X POST \
  -H "Accept: application/vnd.github+json" \
  -H "Authorization: Bearer <YOUR-TOKEN>"\
  -H "X-GitHub-Api-Version: 2022-11-28" \
  
  https://api.github.com/orgs/ORG/actions/runner-groups \
  
  -d '{"name":"gpu-group",
       "visibility":"selected",
       "selected_repository_ids":[123,456],
       "restricted_to_workflows": true,
       "selected_workflows": 
         ["<ORG-NAME>/<REPONAME>/.github/workflows/<WORKFLOW>.yml@main"]
      }'
In the overview depicted in figure 7.1, you can see how many runners are in each group as well as the overall settings per group. Creating or editing a specific group will bring you to the settings shown in figure 7.2. You can configure whether the runners in the group will be available to be used by all repositories (or all organizations on the enterprise level) or only a select subset of them. There is also an option to specify whether the group can be used by public repositories or not. In chapter 6, we have shown the security implications of self-hosted runners. Especially for the use of self-hosted runners on public repositories, it is crucial that you have a secure setup and don’t let anyone create pull requests against your public repository that will directly run against your self-hosted runner! That is why this setting is not enabled by default.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH07_F01_Kaufmann.png)<br>
**Figure 7.1 Runner groups**

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH07_F02_Kaufmann.png)<br>
**Figure 7.2 Changing a runner group**

You can even go a step further and configure the runner group to only be used for specific workflows, as shown in figure 7.3. This can be helpful if you have, for example, a runner with a GPU enabled but you do not want every workflow in a repo to be able to run on that runner, as that could be a waste of resources. There can also be security reasons for separating your runners like this. You can configure one or more workflows that are allowed to use the runners in a group. Adding a specific reference to the workflow is required and has to be in the form of `<organization>/<repository>/.github/workflows/<filename>@<reference>`; wildcards are not allowed. The reference can be any valid git reference, so the name of a branch or tag will work as well as an SHA hash of a commit.

Locking down a runner group to a workflow can be ideal for spinning up a runner on demand by listening to a webhook. To achieve that, configure the group for a specific workflow and a specific revision, which will make this run (and only this run) land on the newly created runner. Configuration of the webhook has been shown in chapter 6. From automation in the webhook, you can create a runner group on demand and lock it down to the workflow that triggered the runner creation, as shown in listing 7.1. Then, create a new runner inside the newly created runner group, which can now only be used by the correct workflow.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH07_F03_Kaufmann.png)<br>
**Figure 7.3 Locking a runner group to a specific workflow**

Note that it is not possible to lock down a runner group directly to a specific team. You can only do that on the repository level, by configuring the repository to be allowed to use the runners in the group.

### 7.1.1 Assigning a runner to a runner group

The group a runner is part of will be configured by default on the creation of the runner. If you do not configure it, the runner will be added to the group named `Default`, which can be used by any repo at the level where the group exists (enterprise or organization). The following listing shows an example of configuring the runner group in the `config` script by passing in the name of the group. A runner can only be assigned to a single group at the same time.

Listing 7.2 Adding a runner to a group during configuration

```sh
./config.sh --url <url> --token <token> --runnergroup <name of the group>
```

When the runner has been created, you can still move it to another runner group by either using the REST API or using the web interface, as shown in figure 7.4. The runner does not even need to be online to be able to move it. The runner will have the security set up immediately after saving the changes and can then be used from the repositories that have access to that group. Any running jobs will finish first with the security rules for the runners when the job started.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH07_F04_Kaufmann.png)<br>
Figure 7.4 Moving the runner to a different group

## 7.2 Monitoring your runners

You can view the available runners on the organization or repository level by going into Settings > Actions > Runners or using the Runner Groups entry in the same menu. For the enterprise-level runners, you can go to Enterprise Settings > Policies > Actions and then open the Runner tab or use the Runner Groups tab. In the runner overview, you find all runners that have been registered successfully with GitHub along with their status. A runner can be in one of four states here:

* *Idle*—Online and waiting for a job to execute
* *Active*—Executing a job
* *Offline*—No communication with the server, meaning the runner could be offline or updating to a newer version of the service
* *Ready*—Used for GitHub-hosted runners, indicating there is no runner online at the moment but the setup is ready to spin up a runner on demand

In the runner overview, you can search for runners with a certain name or use the search query, as shown in figure 7.5.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH07_F05_Kaufmann.png)<br>
Figure 7.5 Checking runner status

Searching can only be done on the part with the runner’s name or by specifying one or more labels to search on:

```sh
team-a label:linux label:xl
```

This code searches for a runner with `team-a` in the name and has both the labels mentioned in the search. Note that this search query is case insensitive and the spaces serve the purpose of breaking between the search commands. Searching with, for example, wildcards in the name is not supported, nor is searching for a part of the label.

The runner group overview (see figure 7.6) provides an overview of the number of runners in that group as well as the security settings on the group but does not give any indication of the status of the runners in the group. This page only allows you to search for the part with the name of the group. That means to monitor uptime and utilization, you will need to implement your own solution.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH07_F06_Kaufmann.png)<br>
Figure 7.6 The runner group overview

### 7.2.1 What to monitor

What you want to monitor is dependent on the type of runners and the setup you have chosen. With, for example, the Actions Runner Controller (ARC), the autoscaling solution from GitHub discussed in chapter 6, you need to monitor two important metrics:

1. Queue time of the jobs
2. Triggering of scaling up and down

If you have a solution of spinning runners up on demand, then the queue time of the jobs is the most important metric to keep track of. This will indicate if your runners are spinning up fast enough to prevent your users from waiting until their job is starting. Especially on *bursty* workloads (large amounts of jobs being queued at the same time), queue time can start to become longer rather quickly if your runners cannot spin up fast enough. Scaling down too fast is also not a great option, as that will potentially create a loop between scaling up and down constantly.

Keeping track of the number of concurrent jobs being executed is interesting, from the perspective of knowing how many jobs, and therefore runners, you need at normal times, but be aware that the queuing of jobs can be very *spikey*, depending on your users. There are always user groups that have nightly jobs scheduled and other groups that schedule those jobs at the beginning of their workday. Depending on how geographically spread out your user base is, this can easily mean a big spike in the middle of the afternoon or evening. Your scaling or just-in-time (JIT) solution needs to be able to handle these spikes gracefully, without scaling out of control for a single user who is trying out the matrix strategy in their workflow for the first time and running it at maximum scale (256 jobs in one matrix) and scheduling those runs every 5 minutes. This can create some serious load on your runner setup (as well as the GitHub environment), and the pertinent question will be whether this single spike means all users will have to wait for the queue to clear up or your solution is set up to handle these use cases efficiently.

Staying with the example of the recommended scaling solution, using ARC, you’ll probably want to either configure this with the job queued webhook and spin up a runner on demand or work with the deployment setup where you configure that you always have a certain number of runners available and let ARC handle scaling up and down when needed. In the second example, ARC will monitor your runners and check the number of runners that are busy every period and, based on configurable rules, will, for example, scale up if over the period of the last 10 minutes, 70 percent of the runners were busy executing a job. You can then indicate to scale up by a percentage of new runners. This can also mean that scaling up for a bursty load can take quite some time! Take an example where you have 50 runners available at any given time as a minimum. You have a rule that looks for 70 percent of runners to be busy and gets evaluated every 10 minutes. If the 70-percent-busy threshold is reached, you scale up runners by 25 percent. With this setup, one or more users schedule 100 jobs that take a while to run—let’s say an hour. Scaling will happen after the first 10 minutes, where 25 percent times 50 runners equals 12 new runners to be started. All existing and new runners are immediately busy executing jobs. It takes another 10 minutes to scale again. The rest of the example can be found in table 7.1. You can see that it takes 40 minutes with this setup to scale to a burst of new jobs getting queued, which are more than the runners you had available. It’s up to you to define the needs of the organization, which can only be done by monitoring the use of your runners.

**Table 7.1 Scaling out runners**

| **Duration (mins)** | **Action**          | **Number of runners** | **Jobs queued** | **Jobs running** | **Percentage busy** |
|---------------------|---------------------|-----------------------|-----------------|------------------|---------------------|
| 0                   | 100 jobs get queued | 50                    | 50              | 50               | 100%                |
| 10                  | Scale out by 25%    | 62                    | 38              | 62               | 100%                |
| 20                  | Scale out by 25%    | 77                    | 23              | 77               | 100%                |
| 30                  | Scale out by 25%    | 96                    | 4               | 96               | 100%                |
| 40                  | Scale out by 25%    | 120                   | 0               | 100              | 83%                 |

Depending on the time it takes to spin up a new runner, you can define a different strategy of scaling as well. If spinning up a runner is rather fast (less than a minute), then your users can likely live with that delay. In that case, it is advisable to work with the webhook and spin up runners on demand, where every time a job is queued, a new runner is created. Spin them up as ephemeral and remove them on completion of the job. You can still have a pool of runners available on standby and create new runners as the jobs come in—that way, you can skip any larger start-up time.

Another strategy for scaling is time based: if your users need the runners mostly during office hours, then you can spin up and down based on that. Create 100 runners at the start of the day, and scale down at the end of the day. These strategies can be combined when using a solution like ARC by configuring multiple scaling rules.

### 7.2.2 Monitoring available runners using GitHub Actions

GitHub Actions is not meant for any sort of monitoring, as there are no guarantees that events will be triggered immediately or that cron schedules will be followed on the second. There can always be some lag in triggering a workflow or a job. That said, since there are no out-of-the-box solutions available from GitHub, you could utilize a workflow that runs and checks whether the expected number of runners are connected. If the number of runners is less than a predefined number, you can trigger an alert into your tool of choice (e.g., Slack or Microsoft Teams). One example is using the free load-runner action (https://github.com/devops-actions/load-runner-info) to get information about the amount of runners available. This action will, for example, give you the number of runners available per label. This can then be combined with your own rules and your own notification channel to trigger an update to your team. An entire workflow example can be found in the readme of the action itself. The downside here is the information can only be loaded on a recurring schedule and cannot be retrieved in real time. While it is not ideal for scaling the runner setup on the fly, this option can at least be used as a starting point for getting some insight into how your runners are being used.

### 7.2.3 Building a custom solution

Another option is to look at the free github-actions-exporter project (https://github.com/Spendesk/github-actions-exporter) and export the usage of actions from the GitHub API into a monitoring solution of your choice on a regular schedule, using the OpenTelemetry output from the exporter. It can be used to export into Prometheus by default, for example. Although the solution has not been touched and updated for a while, the basic premise and setup is still valid. After exporting the data you need into a type of storage, you can create your own dashboards, queries, and alerts. This will give you full control over the solution, but it can take quite some time to prepare a working solution. You can think of Grafana, Prometheus, and others as tools to build your own dashboards and alerts on top of the exported data. The downside here is again that the results will not be available real time, only after the fact when you run a download cycle. The Prometheus setup does this every 30 seconds by default, which can cause some rate limiting problems. This method can still be very useful for gaining insights into the usage patterns of your runners. An example of a Grafana dashboard is shown in figure 7.7.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH07_F07_Kaufmann.png)<br>
**Figure 7.7 An example of a Grafana dashboard**

### 7.2.4 Using a monitoring solution

You have several options when choosing a monitoring solution to integrate with GitHub Actions. DataDog has a paid GitHub integration that will pull information from the GitHub API and give you insights into your GitHub workflows—for example, indicating how long a workflow as well as the individual jobs and steps took to run. For more information, see DataDog’s article on their CI Visibility feature (Chen, 2022, https://mng.bz/PNXn). One important metric it will show you is the queue time of jobs. The DataDog integration does not retrieve any metrics on the runner level at the moment (e.g., how many runners are available or busy at a point in time). We recommend looking at the queue time of your jobs to gain insight on, for example, the number of runners you should have available. This information is included in the DataDog integration.

This solution is also running on a cron schedule to retrieve the information using the GitHub API and will not give you real-time information. You can still learn a great deal of information from your runners’ usage patterns from this setup. This is very helpful when you get started running GitHub Action workflows at scale.

Alternatively, you can use a webhook at the organization or enterprise level to send notifications of jobs getting queued, starting, and completing into a monitoring solution of your choice. This is the best solution for making real-time information available. An example of the hook configuration can be seen in figure 7.8. The webhook can be sent anywhere, as long as GitHub can reach that URL. The payload of the webhook can be ingested by an application like Azure Log Analytics, Splunk, or any other tool that can visualize the JSON-formatted data being sent in. The Splunk app, which, amongst other visualizations, gives you information about the number of workflows being triggered as well as the job outcomes and duration, is a viable option. You can find more information on the app via their website: https://splunkbase.splunk.com/app/5596. The benefit of using Splunk is that the queries have been prewritten and can give you a first overview quite quickly. The downside is that the out-of-the-box dashboards don’t go far enough to properly manage your self-hosted runners. It does not show queue times, for example. Adding your own custom dashboards on the data is straightforward if you are familiar with Splunk. The data that is used and the initial queries can be taken from the existing dashboards and can then be the base of your custom queries and alerts.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH07_F08_Kaufmann.png)<br>
**Figure 7.8 Configuring a webhook to send information about jobs starting**

## 7.3 Runner utilization and capacity needs

When you start creating your own runners, the need for defining the capabilities for them will start to arise. Often, we see people start with rather simple runners: maybe a dual-core processor and 2 GB of RAM. This is fine for most normal workflows, where you lint code or build an application. For some projects, these hardware specs are not enough to complete your workload within a reasonable amount of time. If you are using modern working practices, like CI/CD (discussed in the next chapters), you want your build validation to occur as quickly as possible so that the developers get fast feedback. If they have to wait long for a build to complete, they will start doing something else, which comes with the cost of context switching. Most of the time, you can shorten the time the developer has to wait by adding more hardware capacity, giving the runner more RAM or more CPU cores (or both). This can significantly speed up build times and shorten the feedback cycle for the developers. An example showing a multi-hour workflow job being completed more quickly, as well as the potential savings associated with eliminating hidden developer costs, is outlined in the GitHub blog post “Experiment: The Hidden Costs of Waiting on Slow Build Times” (Somersall, 2022, https://mng.bz/JN6V).

There is no golden rule for finding out how much compute power a workflow job needs. You can monitor you runner environments for their utilization, which will give you a hint on whether adding more power will be of any help. If the entire job only uses 50 percent of your compute, then adding more resources will probably not have any effect. But if the usage spikes close to 90 percent utilization, then it might be worthwhile to try out a bigger runner.

The same goes for jobs that execute on a runner with way too much power: running them on a smaller runner will probably take almost as much time but free up the larger runner for other workloads. It makes no sense to execute a code linting job that takes 30 seconds to run on a big 64-core runner with 32 GB of RAM; that machine can probably be used more effectively.

Monitoring can be done by using your normal monitoring solutions, in the form of agents installed on the runners, which send data to your central monitoring server to review after the fact. Depending on the monitoring solution, you can add additional data fields like the name of the runner, repository, and workflow. With this information, you can correlate the runner utilization to the workflow job that was executed.

Another option is to point your users to the `telemetry` action (https://github.com/runforesight/workflow-telemetry-action) and use it in their jobs. The action will start logging information about step duration, CPU, RAM, disk IO, and network IO. At the completion of the job, the information will be shown in MermaidJS charts in your workflow summary. An example of the CPU metrics is provided in figure 7.9. This action uses tracing of the metrics through NodeJS and, therefore, works across Ubuntu-, Windows-, and macOS-based runners; however, it does not work on container-based jobs.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH07_F09_Kaufmann.png)<br>
**Figure 7.9 CPU utilization of the runner**

## 7.4 Monitoring network access

You need to be aware of what the runners are doing inside of the environment you have set up for them. By default, the runners need access to the internet to be able to download and run actions. If the action is based on a container, that image will need to be downloaded as well. As most container actions from the public marketplace use a local Dockerfile, this will need to be built at run time as well, with all the dependencies it needs. The default setup of the runner also includes an auto-update mechanism, which will also require internet access. If you are running GitHub Actions against Enterprise Server, the runner updates will be downloaded from the server itself.

The main reason to look at outgoing connections is maintaining security. You want to be aware of what actions and scripts are doing on your runners to see if they match your expectations. For example, why would an action that is intended to lint your code for guidelines need to connect to a third-party API endpoint? It would be weird if it did, as that does not match the expectations of a linter. As an action is built on top of an ecosystem, like npm, attack vectors for the action are numerous. Therefore, you need a way to monitor and limit networking access on top of vetting the actions before they get to your end users.

### 7.4.1 Monitor and limit network access

The runner service itself has no options for monitoring or limiting network access. The whole setup assumes internet access is available and that the runner can always download the action repositories and all the necessary dependencies. That means you will need to set up your own monitoring solution. The options for this depend heavily on the platform and setup that is chosen. If you execute the runner on a virtual machine in a cloud environment, you can set up networking monitoring and rules on that level. This will give you some insights, but stopping outgoing connections can become more cumbersome as the usage of your runners increases. Segmenting your runners into different networking segments can be done by deploying them differently and giving the runners labels that match the networking capabilities. You can also configure the runner groups for certain repositories with only the access those repos need for their type of workloads.

Additionally, there are vendor solutions, like StepSecurity (https://www.stepsecurity.io/), which can help you monitor the outgoing connections from your runners by installing an agent at run time. That agent is called *harden-runner* and is free to use for public repositories. For private repositories, it is a paid product. The harden runner starts with an initial testing phase to gather the connections being made by a job and logging those connections to the software as a service (SaaS) of the product. After knowing and analyzing the connections that are made, you can add an allow list to the workflow and lock down the connections it can make. The solution from StepSecurity works by using a custom Linux DNS setup and needs `sudo` rights, which means it does not work on macOS or Windows runners. Container support is also not present at the time of writing. There is also support for the ARC setup, where tooling is used on the Kubernetes cluster level so that not every workflow needs to install the harden runner by itself. This greatly improves the usability for end users. For ARC support, you will need a paid license.

An example of configuring the `harden-runner` action to analyze the outgoing network connections being made from the job can be found in listing 7.3. By running this workflow, you will learn that the `setup-terraform` action will download the binaries from https://releases.hashicorp.com, which is expected. You will also learn that running `terraform version` also makes an outgoing connection to https://checkpoint-api.hashicorp.com, as it is also checking if there is a newer version to download and will log a warning in that case. The `harden-runner` setup can then give you fine-grained control over the connections you want to allow. Listing 7.4 shows an example where all outgoing connections will be blocked (and logged), except for the endpoints in the *allow* list. The code used for the [agent][1] is written in Go and available open source.

**Listing 7.3 Configuring harden-runner**

```yml
name: harden runner demo
on:
  workflow_dispatch:
  
jobs:
  demo:
    runs-on: ubuntu-latest
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@v2.11.0
        with:
          egress-policy: audit # TODO: change to 'egress-policy: block' after couple of runs
          
      - uses: actions/checkout@v4
      
      - uses: hashicorp/setup-terraform@v3
      
      - run: terraform version
Listing 7.4 Using harden-runner with a block policy

- name: Harden Runner
  uses: step-security/harden-runner@v2.11.0
    with:
      egress-policy: block
      allowed-enpoints: >
        api.nuget.org:443
        github.com:443
```

You can also use your own networking setup to limit the outgoing connections from your runners. If you are using ARC on Kubernetes, as described in chapter 6, then it is possible to use egress control using network policies around your runners to allow or deny certain traffic to connect to the internet or limit it to certain endpoints. Tools to look at for this include, for example, Cilium and Calico.

If you host your runners in your own networking setup, it is possible to segment the networks for the runners and only configure certain endpoints to be used. Having a pool of runners ready for each type will create some overhead as you need to have a warm pool of runners available for each group. Next to that, you need to handle scaling up and down for each pool yourself.

### 7.4.2 Recommended setup

There is a tradeoff between being very restrictive for your runners and what they are capable of doing, in terms of connecting to external endpoints and your GitHub environment. Connections back to GitHub have to be made in any case, and additionally, your users will want to use GitHub Actions and download them from a marketplace.

Our recommendation is to use a declarative style in your workflows, like, for example, StepSecurity uses and have the users specifically configure to which endpoints they need to make connections. This will prevent data from leaking out to third-party endpoints without being aware of it. With the `block` policy from StepSecurity, any extra connection that is made will be blocked initially and logged centrally so that your security team can keep track of new connections being requested. This will greatly improve your runner and workflow security!

## 7.5 Internal billing for action usage

Self-hosted runners come with setup costs, hosting costs, and maintenance costs. Even if you use them on GitHub Enterprise Cloud, the usage for self-hosted runners is not included in the usage reports. It can be very helpful to show teams how they have been using the runners over time and make them more aware of the costs of having them online all the time. Those costs can be split between hosting the machines and the amount of energy used—and, thus, the CO2 they generate. Users should consider both aspects when determining if they really need to use five jobs in parallel or if it would be better to run the same steps in sequence (and, in doing so, use less concurrent machines).

For the usage aspects, you can either use the information already available in your monitoring tool (e.g., Splunk) and separate the information out by repository or team. If you don’t have a monitoring tool in place, you can also use the `actions-usage` tool (https://github.com/self-actuated/actions-usage). This uses the GitHub API to get actions usage information for each workflow as well as an overview, like the example shown in figure 7.10. Most tools only call the GitHub API on the workflow level and calculate the duration of the entire workflow. It is possible to do the same on the job level, but that will not include extra information (like the used label for the job). That is why most tools do not make the extra API calls to load that information as well. This also means it is harder to make the split between GitHub-hosted and self-hosted runners, if you mix these in the same repository or workflow! You could take the extra step of getting the information on the job level, as that will include the commit SHA of the workflow definition. You can then download that version of the workflow and parse the definition yourself.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH07_F10_Kaufmann.png)<br>
**Figure 7.10 Action minutes overview report**

Once you have the action minutes used by repository, you can calculate the price of the runs by multiplying the minutes by a predefined cost. Combine that with the used network traffic, and you have a more complete picture of all the things the users are doing in their repositories and workflows.

Rolling up the repositories can be done on a team level or any other level if you add topics to the repository and use that to slice information into groups. Showing this information in, for example, a monthly report or a dashboard can help the users become more aware of what they are actually doing in their workflows. We have seen examples where a repository of 300 MB was cloned four times in the same workflow file, getting seven lines of shell script and executing it. The seven lines of script were, in total, 11 bytes. By doing a shallow clone of only the script, we saved 1.2 GB of network traffic in every single workflow run (this workflow was used everywhere!).

## Summary

* You can gain valuable insights by monitoring your runners for availability and the ways they are being used.

* It is important to use runners sized appropriately for the job and to report this information back to your end users.

* Runners can be configured to only be used by certain repositories, by placing them into runner groups.

* Runners can be moved between runner groups but can only be part of a single group.

* Monitoring your self-hosted runners is important so that you can determine whether there are enough runners available for your users.

* Even with scaling solutions, you still need to monitor for scaling actions to determine whether you’re scaling in and out efficiently and scaling up at the appropriate speed.

* Information on how your repositories are using the runners is not a GitHub feature out of the box. Existing open source solutions have their pros and cons. They can be used to get started loading the information, but more-specific information, like runner labels, is necessary for a full overview.

* Reporting usage information to your users enables them to consider the ways they are using your runners more critically—should they really clone the repo every time to run a simple script, or can this be done more intelligently?

[1]: https://github.com/step-security/agent
