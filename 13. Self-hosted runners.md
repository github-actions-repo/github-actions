- [Self-hosted runners](#self-hosted-runners)
  - [6.1 Setting up self-hosted runners](#61-setting-up-self-hosted-runners)
    - [6.1.1 Runner communication](#611-runner-communication)
    - [6.1.2 Queued jobs](#612-queued-jobs)
    - [6.1.3 Updating self-hosted runners](#613-updating-self-hosted-runners)
    - [6.1.4 Available runners](#614-available-runners)
    - [6.1.5 Downloading actions and source code](#615-downloading-actions-and-source-code)
    - [6.1.6 Runner capabilities](#616-runner-capabilities)
    - [6.1.7 Self-hosted runner behind a proxy](#617-self-hosted-runner-behind-a-proxy)
    - [6.1.8 Usage limits of self-hosted runners](#618-usage-limits-of-self-hosted-runners)
    - [6.1.9 Installing extra software](#619-installing-extra-software)
    - [6.1.10 Runner service account](#6110-runner-service-account)
    - [6.1.11 Pre- and post-job scripts](#6111-pre--and-post-job-scripts)
    - [6.1.12 Adding extra information to your logs](#6112-adding-extra-information-to-your-logs)
    - [6.1.13 Customizing the containers during a job](#6113-customizing-the-containers-during-a-job)
  - [6.2 Security risks of self-hosted runners](#62-security-risks-of-self-hosted-runners)
  - [6.3 Single-use runners](#63-single-use-runners)
    - [6.3.1 Ephemeral runners](#631-ephemeral-runners)
    - [6.3.2 Just-in-time runners](#632-just-in-time-runners)
  - [6.4 Disabling self-hosted runner creation](#64-disabling-self-hosted-runner-creation)
  - [6.5 Autoscaling options](#65-autoscaling-options)
    - [6.5.1 Autoscaling with Actions Runner Controller](#651-autoscaling-with-actions-runner-controller)
    - [6.5.2 Communication in ARC](#652-communication-in-arc)
    - [6.5.3 ARC monitoring](#653-arc-monitoring)
  - [Summary](#summary)

# Self-hosted runners

This chapter covers

* Setting up self-hosted runners
* Securely configuring your runners
* Using ephemeral runners
* Choosing autoscaling options
* Setting up autoscaling with Actions-Runner-Controller

In chapter 5, we saw how we can use GitHub-hosted runners, when they are useful, as well as how billing works for those hosted runners. You can also install your own runners in your own environments, which are referred to as *self-hosted runners*. Creating self-hosted runners gives you full control over their execution environment, like placing it inside of the company network or adding specific hardware or software capabilities. Self-hosted runners can also be beneficial from a cost perspective, since you do not need to pay for any action minutes for jobs that run on self-hosted runners. There is, of course, a cost associated with hosting, setup, and system administrative tasks you will have to complete to keep the environments you host the runners on up to date and secure.

Self-hosted runners can prove beneficial by allowing you to run a self-hosted runner inside of your company network, enabling the runtime to connect to a database service to run certain integration tests or deploy into your production environment, which cannot be accessed from outside the company perimeter. Maybe you need a GPU-enabled machine for certain jobs, or perhaps, you need certain (larger) Docker containers; installing a self-hosted runner on a machine that already has those containers downloaded and precached can save a lot of time and network bandwidth.

## 6.1 Setting up self-hosted runners

Self-hosted runners can be set up by installing the runner application and following the steps from the documentation for the OS that will be hosting the service. The service itself is open source and can be found in the following repository: https://github.com/actions/runner. This repository also hosts the releases of the application. The application is based on the .NET core runtime and can be executed on a large number of operating systems and processor types, including x86, x64, and ARM processors as well as on Linux, Windows, and macOS. That means you can even run the service inside of a Docker container or on a Raspberry Pi!

The supported operating systems for self-hosted runners can be found in table 6.1. For the current list of supported systems, check the [documentation][1].

**Table 6.1 An overview of supported operating systems for self-hosted runners**

| **Operating system** | **Supported**                                 |
|----------------------|-----------------------------------------------|
| Linux                | Red Hat Enterprise Linux 7 or later           |
|                      | CentOS 7 or later                             |
|                      | Oracle Linux 7                                |
|                      | Fedora 29 or later                            |
|                      | Debian 9 or later                             |
|                      | Ubuntu 16.04 or later                         |
|                      | Linux Mint 18 or later                        |
|                      | openSUSE 15 or later                          |
|                      | SUSE Enterprise Linux (SLES) 12 SP2 or later  |
| Windows              | Windows 7 64-bit                              |
|                      | Windows 8.1 64-bit                            |
|                      | Windows 10 64-bit                             |
|                      | Windows Server 2012 R2 64-bit                 |
|                      | Windows Server 2019 64-bit                    |
| macOS                | macOS 10.13 (High Sierra) or later            |

To get started installing the runner, you will need to have an environment that is supported by the .NET core version (see the docs at: https://github.com/actions/runner for the current version). The .NET core does not need to be preinstalled; the runner is self-contained. It also includes the two most recent versions of the Node binaries it supports, as most of the public actions will need Node to execute. To run the checkout action, you will need to have a recent version of Git installed.

If you want to run Docker-based actions, you will also need to have Docker installed with the runner installed on a Linux machine. Windows and macOS are not supported for running Docker-based actions.

The environment also needs to be able to connect either to GitHub or a self-hosted GitHub Enterprise Server. On Linux, you will also need an account to be able to run the service as `root`, so you will need `sudo` privileges. On Windows, you will need to have `administrative` privileges to configure the runner as a service. The service is installed by downloading the runner and executing the configuration, which tells it the following information:

* *To which GitHub service this runner needs to connect*—It can either be github.com or against your own GitHub server. This cannot be changed after installation.

* *For which hierarchical level this runner is created*—A runner can be linked to an entire enterprise, a specific organization, or for a single repository. This setting cannot be changed after installation.

* *A configuration token used for the installation*—The token can be generated by a user (it is shown in the GitHub UI by default) and is only valid for one hour. You can only use a token once, only during installation. You can create an installation token through the REST API on demand by sending a POST request to https://api.github.com/orgs/<ORG>/actions/runners/registration-token. The token in the result will also be valid for only one hour. The expiration date is also present in the response.

* *The name of the runner*—This will default to the hostname, and it cannot be changed afterward.

* *The runner group to place this runner in*—This will default to the runner group named default. It can be changed afterward, as the runner itself has no idea what group it belongs to after the installation; this is all stored on the GitHub side. With runner groups, you can allow a group of runners to be used on certain repositories. This will be explained in more detail in chapter 7.

* *The labels that will be associated with this runner*—You can add more labels through the UI or API later on, as the runner itself has no idea which labels are assigned to it. That configuration is stored on the GitHub side, so it can be used from that end to find the appropriate runner to send the job to when queued. There is no upper limit on the number of labels you can add, so you can be as specific as you prefer. The only restriction is that the label cannot be longer than 256 characters and cannot contain spaces.

The following listing provides an example of downloading the runner software from a GitHub release and extracting it to get started.

**Listing 6.1 Installation script for creating a runner on Linux**

```sh
# Create a folder.
mkdir actions-runner && cd actions-runner
  
# Download the latest runner package.
curl -o actions-runner-linux-x64-2.305.0.tar.gz -L https://github.com/actions/runner/releases/download/v2.305.0/actions-runner-linux-x64-2.305.0.tar.gz
  
# Optional: Validate the hash.
echo "737bdcef6287a11672d6a5a752d70a7c96b4934de512b7eb283be6f51a563f2f  actions-runner-linux-x64-2.305.0.tar.gz" | shasum -a 256 -c
  
# Extract the installer.
tar xzf ./actions-runner-linux-x64-2.305.0.tar.gz
```

Then, the next listing contains the script for configuring the runner for an organization with only the default token that is present in the GitHub UI. This token is valid for one hour.

**Listing 6.2 Configuring and starting the runner**

```sh
# Create the runner and start the configuration experience. 
./config.sh --url https://github.com/devops-actions --token ABONY4PKE6CXIW5YZREB3EDES4LLG
  
# Last step, run it!
./run.sh
```

Some extra configuration parameters that are not required include the following:

* `work`—Overwrites the default location where the downloaded work will be stored. This defaults to the `_work` directory relative to the runner application directory.

* `replace`—Indicates whether you want to replace an existing runner with the same name. This defaults to `false`.

On Windows, the configuration script will ask you if you want to execute the runner as a service so that it will start with the environment. On Linux, you will have to configure the service yourself using the svc.sh script. See the following listing for an example.

Listing 6.3 Installing the runner as a service on Linux

```sh
# Installs the service; the parameter USERNAME is optional to run as a different user than root.
sudo ./svc.sh install 
  
# Starts the service
sudo ./svc.sh start
  
# Checks the status of the service
sudo ./svc.sh status
  
# Stops the service
sudo ./svc.sh stop
  
# Uninstalls the service
sudo ./svc.sh uninstall
```

To remove and deregister the service on Windows, you can run the `config` command again with the `remove` parameter. The token needed to deregister is the same type of token as with the installation: a one-time token, generated specifically for (de)registration at that level in the GitHub environment (enterprise/organization/repository). The token that you use has to come from the same configuration point you used for the registration, or else the removal command will fail. So get a token from the same enterprise, organization, or repository where you registered the runner (see figure 6.1).

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH06_F01_Kaufmann.png)<br>
**Figure 6.1 Deregistering and removing a runner**

After configuring the service, you can either start the process as a service (so that it will always be running and ready to receive work) or start it as a one-time process. As a one-time process, it will announce itself to GitHub, wait for the work to come in, and then stop. It will also not be started together with the operating system when not configured as a service. An example of a running service that is waiting for work and then executes a job is shown in figure 6.2.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH06_F02_Kaufmann.png)<br>
**Figure 6.2 A runner service executing work**

If the runner is configured as a service, you can also check its connectivity back to GitHub by running the following command:

```sh
.\run.cmd --check --url <url> --pat <personal access token>
```

You need a personal access token (PAT) because the runner does not have this authentication information available to connect back to the URL.

The runner will show up in the runner list at the corresponding level to the one it was created for (enterprise/organization/repository) under Settings > Actions > Runners (see figure 6.3). In this view, you can search for runners with a certain label by using the search box and using, for example, this search query: *label:self-hosted*.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH06_F03_Kaufmann.png)<br>
**Figure 6.3 Runner overview**

### 6.1.1 Runner communication

The runner communicates with GitHub by setting up an outgoing HTTPS connection. The communication is created as what is referred to as a *long poll connection*; it asks GitHub if there is work queued to be executed for this specific runner, and then it waits for 50 seconds for a response, before the connection is severed. Immediately after closing the connection, a new connection is started that does the same thing, and so on, until the runner is completely stopped. The nice part about this setup is that you can configure the runner anywhere, as long as the firewall is open for outgoing connections over port 443. There is no inbound connection to be made from GitHub back into your network.

The runner itself has no knowledge of the GitHub side of the connection. For example, it does not know for which repositories it is configured to run, the GitHub organizations that can use it, or if it has been set up on the enterprise or repository level. It only knows the GitHub URL it needs to use to ask for work. There is no GitHub user associated with the runner itself. A runner also has no idea what kind of environment it is running in. During installation, it checks the type of operating system being used (e.g., Linux, Windows, or macOS) and the CPU architecture of the environment (e.g., x64, ARM32, or ARM64), and then it sends this information to GitHub as labels that can be used for jobs to target a runner. The labels can later be changed on the GitHub side, since the runners have no idea what labels are assigned to them.

The runner installation will create two files that are important for its communication back to GitHub. In listing 6.4, you’ll find the content of the .runner file in the application folder of the installed runner. As you can see, it is stored as a JSON file with settings for the `agentId` and `agentName`, together with the settings for the runner group (pool) it was configured with. Here, you also find the server being used and the GitHub URL that was used during configuration. If you move the runner between runner groups, this information will not be updated, as it’s only written when configuring the runner. The `gitHubUrl` property does have an owner/repo in the URL, but this is only used to ask the GitHub environment for work.

**Listing 6.4 The content of a .runner file**

```json
{
  "agentId": 23,
  "agentName": "ROB-XPS9700",
  "poolId": 1,
  "poolName": "Default",
  "serverUrl": "https://pipelines.actions.githubusercontent.com/f2MWTcGQc8C3bs21IjVQc2ABCDBpRsWJjinZU0MNTxx0PSYdbu",
  "gitHubUrl": "https://github.com/GitHubActionsInAction/demo-actions",
  "workFolder": "_work"
}
```

In listing 6.5, you can find the content of the .credentials file where a longer-lived authentication token is stored after the runner is registered to GitHub with the registration token in the config command.

**Listing 6.5 The content of a .credentials file**

```json
{
  "scheme": "OAuth",
  "data": {
    "clientId": "21ecc1ca-2d1a-4c44-abcd-309480c44a33",
    "authorizationUrl": "https://vstoken.actions.githubusercontent.com/_apis/oauth2/token/e234a9b7-bd5a-acec-b7cb-b5c40b459af4",
    "requireFipsCryptography": "True"
  }
}
```

The OAuth credentials that are used to authenticate the connection to GitHub are stored in the .credentials_rsaparams file, which is encrypted on Windows with an RSA private key with 2,048-bit-length encryption and can only be read on the local machine. On Linux, this file is not encrypted and can be copied over to another machine and start the runner process there. The file is needed for runners that are expected to reboot (e.g., after upgrading) and then register themselves again. It is also used to refresh the long polling connection that times out after 50 seconds.

The one thing you can do with these credentials is execute the runner service and wait for an incoming job to execute. Having this file available for reading from the user that is used to execute the runner is considered a security risk. The job could read all the information and start a new runner elsewhere with the same configuration. This setup is there for backward compatibility reasons. The recommended configuration for the runners is using the just-in-time (JIT) setup discussed in section 6.3.2. The JIT setup uses the same files, but the token used for configuration is only valid once.

Since the runner communication is an outgoing channel from the runner to the GitHub environment, there are events that happen when the communication stops. When there is no communication from the self-hosted runner to GitHub for more than 14 days, the runner will be removed from the listing and will need to be reconfigured before it is allowed to reconnect. When the runner is configured as ephemeral, it will be removed after 1 day of noncommunication.

To be able to communicate with GitHub, you must ensure that certain hosts can be reached from the runner environment. You can find the full list in the [documentation][2]. Some interesting hosts are shown in table 6.2.

**Table 6.2 Hosts that the runner needs to be able to reach**

Purpose

| **Purpose**                                  | **Hosts**                                          |
|----------------------------------------------|----------------------------------------------------|
| Essential operations                         | github.com                                         |
|                                              | api.github.com                                     |
|                                              | *.actions.githubusercontent.com                    |
| Downloading actions                          | codeload.github.com                                |
| Uploading/downloading job summaries and logs | actions-results-receiver-production.githubapp.com  |
|                                              | productionresultssa*.blob.core.windows.net         |
| Runner version updates                       | objects.githubusercontent.com                      |
|                                              | objects-origin.githubusercontent.com               |
|                                              | github-releases.githubusercontent.com              |
|                                              | github-registry-files.githubusercontent.com        |
| Uploading/downloading artifacts and cache    | *.blob.core.windows.net                            |

### 6.1.2 Queued jobs

When a job is queued for a certain combination of labels, it will stay in the queue if there is no runner online that matches all the labels the job is targeting. An example of a queued job with the labels that were targeted can be found in figure 6.4. The maximum duration self-hosted runners can be queued is 24 hours. If there is no runner available within this period, the job will be terminated and a cancelation message will be sent to the user that triggered the job.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH06_F04_Kaufmann.png)<br>
**Figure 6.4 A queued job waiting for a runner to become active with the self-hosted label**

Currently, there is no API or user interface that provides an overview of all the jobs that are queued for either the enterprise, organization, or repository level. You can only load that for each workflow using the API or for an entire repository, as shown in figure 6.5, where the overview has been filtered using the is:queued query.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH06_F05_Kaufmann.png)<br>
**Figure 6.5 An overview of queued workflows for a repository**

### 6.1.3 Updating self-hosted runners

Self-hosted runners will automatically check with each job they execute if there is a new version of the runner available, either by calling the public GitHub repository where all runner releases are stored (https://www.github.com/actions/runner) or calling the GitHub Enterprise Server, if you are using it. If the runner has not been used for seven days, it will also check for updates and run them if needed. New releases are created by GitHub when needed, which has been almost once a month in the past. The updates contain both fixes and updates. When an update is available, the runner will download it and install it before a new job is accepted. In case you host your runners in a locked-down environment with, for example, no direct internet connection, you will need to keep the runners up to date yourself by pulling in updates regularly in your setup environment. In that case, also configure the runner with the `disable-autoupdate` parameter.

### 6.1.4 Available runners

You can find out which runners already have been configured for your enterprise, organization or repository by navigating to Settings in that level and then to Actions > Runners (see figure 6.6 for an example). Here, we can see which GitHub-hosted runners are available as well as the labels that are available for those types of runners. If there are any runners executing a job, they will be visible in the Active Jobs panel.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH06_F06_Kaufmann.png)<br>
**Figure 6.6 Available runners**

### 6.1.5 Downloading actions and source code

When there is work queued for a self-hosted runner, the runner will first download the definition of the work that needs to be done from GitHub and then start executing it. It will download the job definition and then extract all GitHub Actions statements that are included directly in the job definition. The next step is to download the repositories of the necessary actions by going to the GitHub API and then to download the correct version of the action repo as a zip file. Each action (and version used) will be stored in the subfolder `_work\_actions\actions\<action-name>\<version-reference>\` so that it only needs to be stored on disk once per job. See figure 6.7 for a screenshot of the runner folder on disk, with the actions/checkout action downloaded and a `v2` folder as the version tag. Here, you can also see that the entire repository is downloaded but not as a Git repo (the `.git` folder is missing). That also means every version you use in the job that is executed will get its own version folder as well.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH06_F07_Kaufmann.png)<br>
**Figure 6.7 A runner action folder on disk**

If the action is running a Docker container, the runner will either download the Docker image or start building the included Dockerfile, depending on the setup of the action. Using a prebuilt image can significantly save time executing the action, since it will skip the time needed to build the action. Also note that the image will be built for every single run that the runner executes.

The runner uses the URL that was entered during the installation of the application to download the actions. It will suffix this URL with the actions’ using statement to get a link to the action repository it needs to download. This means it will use www.github.com when connected to GitHub in the cloud or the URL to your GitHub Enterprise Server when connected to a server.

In the case of composite actions or reusable workflows, the runner will download the definition to make sure it exists, but it only expands these configurations and downloads those actions if and when the step or job is executed. This way, the runner only downloads what is needed. Keep in mind that the `_work\_actions\actions\` folder will be cleaned at the start of each job the runner executes to prevent any problems when an action stores data that might get overwritten during job execution in these folders.

When downloading repositories with the `actions/checkout` action, a new directory is created in the _work folder with the name of the repository where the executing workflow is defined and then a folder with the name of the repository that is checked out. Usually, these are the same, so in the example in figure 6.8, you end up with `demo-actions/demo-actions`, as that is the repository we are working with. You can also see that this is an actual Git repository, as the .git folder is there. This gives you the option to switch branches; create new commits and push them back upstream; or work with any tool that uses the Git repo information, like the GitHub CLI, which uses this to execute commands like creating issues and pull requests from the current repository.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH06_F08_Kaufmann.png)<br>
**Figure 6.8 Actions/checkout folder creation**

### 6.1.6 Runner capabilities

The runner gets it capabilities from the environment it is installed in; if there is software installed in the runner, the job that is executed can make use of that software. The environment defines the compute power the runner has, depending on how much RAM, CPU, and network capabilities it has. If there is a GPU available for the environment, the runner will automatically pick that up as well. If you want to execute a Docker-based action, you will need to install Docker on the host. Be aware that a runner can only run a single job at the same time. It is possible to install multiple runners in the same environment, but from a security perspective, this is not recommended, as concurrent jobs can then influence and interfere with each other, since the runner will have access to the entire environment.

When indicating runner capabilities, it is a best practice to add them as labels to the runners so that users can target the capabilities they need. For example, if there is a GPU available, add the label `gpu`. You can also run with a default `self-hosted` label on all runners, and if you need a runner with more RAM available, target the runners, for example, with the label `xl`. You could even go so far as to have labels for both large RAM (`ram-xl`) and for large disk size (`disk-xl`). This will also guide users toward considering what they actually need and specifying that with the labels they target; a simple linter job should not have to run on a runner with 64 GB of RAM available if it does not need that much power. To make this message even clearer, you can bill for action minutes internally, using your own cost per minute for the different runner types. See chapter 7 for more examples on internal billing.

### 6.1.7 Self-hosted runner behind a proxy

Proxy support is available for self-hosted runners. You can either use the standard environment variables (`https_proxy`, `http_proxy`, and `no_proxy`) to pass in the information, or use an .env file in the runner application folder, containing the information shown in the following listing. If you are also using Docker-based actions, you need to update the Docker configuration by adding the proxy settings to the ~/.docker/config.json file.

Listing 6.6 Proxy configuration in an .env file

```env
https_proxy=http://proxy.local:8080
no_proxy=example.com,myserver.local:443
https://username:password@proxy.local
```

### 6.1.8 Usage limits of self-hosted runners

Even though GitHub neither restricts the number of concurrent jobs executed on self-hosted runners nor enforces the normal timeouts for jobs, there are still some limits to be aware of when using self-hosted runners:

* The total workflow duration cannot be longer than 35 days. This includes job execution and time spent waiting and seeking approvals.

* The maximum queue time for a job on self-hosted runners is 24 hours. If the job has not started executing within this time frame, it will be terminated.

* A job matrix can generate a maximum of 256 jobs per workflow run. If it generates more, the workflow run will be terminated and fail to complete.

* No more than 500 workflow runs can be queued in a 10-second interval per repository. Additionally, queued jobs will fail to start.

### 6.1.9 Installing extra software

You are in full control of what you install on self-hosted runners. After installation and adding it to the $PATH, you can use the software in your workflow definitions. You can either preinstall the software on the runner or install it on demand. In general, you do not want to make your job definitions dependent on a specific type of runner so that you have more freedom to switch runners. For the job itself, where it is running is inconsequential; if the job is self-contained, it will install all necessary software on its own (e.g., it will download the latest Node version and install it). If the job needs it, it can specify the dependency itself:

```yml
steps:
 - name: Install Node with version
   uses: actions/setup-node@v3
   with:
     version: 18.*
  
 - uses: actions/checkout@v3
  
 - name: use the CLI
   run: node --version # Check the installed version.
```

If you decide to start preinstalling software on the runner itself, like in a virtual machine setup, the general recommendation is to keep your runners as uniform as possible. What we often see is that different user groups (e.g., teams) have different needs. When the runner definition starts to diverge, it can become unclear to the users what to expect of the self-hosted runners. The best practice is then to add the installed software or capability as a label to the runner so that the users can specify the right label to target the right runner. Keep in mind that jobs will only be queued on a runner if all the labels on the job match, as in the following example:

```yml
runs-on: [self-hosted, gh-cli, kubectl]
```

This job can only run on a runner that has all three labels. Some of the most commonly used software includes system tools that are often used in jobs:

* The GitHub CLI
* Libraries that help you work with JSON or YAML, like `jq` or `powershell-yaml`
* Cloud-specific CLIs, SDKs, or other tools (e.g., the AWS Cloud Development Kit or the Azure CLI)
* SDKs for the company’s most commonly used coding languages
* Caching the most commonly used Docker images to save bandwidth costs and time downloading images
* Container tooling (e.g., Docker, BuildX, and Buildah) and Kubernetes tooling (e.g., Helm and kubectl)
* Mobile application tooling (e.g., Android Studio and Xcode)

Another option for this setup is to have a list of container images that your users can configure when they need it. They then configure the use of the image with the container keyword on the job level (see the following listing for an example). All the steps in that job will run inside of the container, with any tool that you have installed in that container as well.

**Listing 6.7 Running the entire job in your own container**

```yml
jobs:
  run-in-container:
    runs-on: ubtuntu-latest
    container: alpine:3.1.2
    steps:
      - uses: actions/checkout@v4
```

We often get the question of how to get the same images for the runner as the VM image that GitHub uses for their hosted runners. For licensing reasons, GitHub cannot distribute so-called golden images that already have everything preinstalled. They do give you the installation scripts to run and build your own image from the source code in the runner images repository. You can find the scripts to get started in this [repository][3]. All the prerequisites to get started can be found in the same documentation.

### 6.1.10 Runner service account

The runner gets the rights to its environment from the way it was installed. On Windows, you can configure it to run as a service with a certain service account. It will then have access to everything on the environment that the service account has access to, including any networking access.

For Linux and macOS, the default setup is to run the service as root, though you can configure it to use a nonroot account. Be aware that this often causes some problems with actions or jobs that run inside of a container on nonephemeral runners. The container runs with its own account setup, which is often root. The GITHUB_WORKSPACE folder will get mounted inside of the container. When the steps executed inside the container change a file or folder in the workspace, those files will get root-level access attached to them as well. Any subsequent cleanup of those files afterward on the runner will fail if the runner is not executing as root.

### 6.1.11 Pre- and post-job scripts

The runner service can be set up with an environment variable that holds the path to a script that can either run as a step at the beginning of a job or as the last step of the job. This can be used to prepare the runner environment with internal configurations, and we have used it to configure default read-only accounts to internal package managers and Docker registries. To configure the pre- and post-job scripts, you need to save a script in a location the runner account will have access to and then configure the corresponding environment variables for each hook:

```yml
ACTIONS_RUNNER_HOOK_JOB_STARTED
ACTIONS_RUNNER_HOOK_JOB_COMPLETED
```

Another option is storing these values as key–value pairs in an .env file inside the runner application directory. The value of the settings needs to be the full path to the script that can be executed. If the runner account does not have access to that path, the set-up runner step will fail.

When the startup hook is configured, it will show up on the logs of the jobs that are executed on that runner as an extra step at the beginning of the job. An example is shown in figure 6.9. Note that the extra step runs after downloading all the action definitions. The job-completed hook does the same thing, except as a last step at the end of the job.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH06_F09_Kaufmann.png)<br>
**Figure 6.9 Set-up runner step**

The environment variables can be set at any time, including after the installation, as long as they have been set before the next job executes. Any changes during a job execution will not be used.

The scripts are executed synchronously for the job run, as a normal step. If the exit code for the script is nonzero, the step will fail, and the job will stop executing. Additionally, these scripts will not have a timeout applied to them from the runner, so if needed, you will need to configure a timeout handler inside the script itself. The scripts also have access to the default variables, as they are treated as a normal step in the job. That means you have access to variables like the `GITHUB_WORKSPACE` or `GITHUB_TOKEN`.

### 6.1.12 Adding extra information to your logs

There is support for showing extra information to your logs by placing a file called .setup_info in the runner’s application folder. See listing 6.8 for the contents GitHub uses for hosting their runners. The information is grouped with a tile for the group, which will result in grouped information in the setup job step in each run on this runner. The result is shown in figure 6.10. Note the use of \n for adding breaks in the output and start a new line.

**Listing 6.8 The contents of the .setup_info file on GitHub-hosted runners**

```json
[
  {
    "group": "Operating System", 
    "detail": "Ubuntu\n22.04.2\nLTS" 
  },
  {
    "group": "Runner Image",
    "detail": "Image: ubuntu-22.04\nVersion: 20230702.1.0\nIncluded Software: https://github.com/actions/runner-images/blob/ubuntu22/20230702.1/images/linux/Ubuntu2204-Readme.md\nImageRelease: https://github.com/actions/runner-images/releases/tag/ubuntu22%2F20230702.1"
  },
  { 
    "group": "Runner Image Provisioner", 
    "detail": "2.0.238.1" 
  }
]
```

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH06_F10_Kaufmann.png)<br>
**Figure 6.10 Results of the .setup_info file**

### 6.1.13 Customizing the containers during a job

With the keyword `container`, users can specify that their job will run inside of a Docker container. The runner has default setups for the `docker create` and `docker run` commands it executes to get the container set up and running. You can overwrite the default commands with your own custom JavaScript file that runs when a job is assigned to the runner but before the runner starts executing the job. This allows you to add custom volume mounts, configure your private container registry, or always run with a sidecar container. To configure the customization, store a reference to the script you want to run in the `ACTIONS_RUNNER_REQUIRE_JOB_CONTAINER` environment variable or store this configuration in an .env file in the runners’ application folder as a key–value pair, where the value is the path to the JavaScript file.

Be aware that the script will run synchronously and, thus, will block the execution of the job until the script completes. There is also no timeout for the script, so you will need to handle a timeout mechanism inside the script. The script will run in the context of the runner service with the corresponding system and networking access.

The following configuration commands are available:

* `prepare_job`—Called when a job is started
* `cleanup_job`—Called at the end of a job
* `run_container_step`—Called once for each container action in the job
* `run_script_step`—Runs any step that is not a container action

Each command has its own definition file, with the filename being the name of the command and the JSON file extension. Another option is to use an index.js file that can trigger the correct command when it is called. Examples for setting up Docker, HookLib, and Kubernetes projects can be found in the following GitHub example [repository][4].

## 6.2 Security risks of self-hosted runners

Running jobs on self-hosted runners comes with a risk as well. The self-hosted runner might have too much access to your network and could be used for network traversal attacks (i.e., travel to other machines in the network either for reconnaissance or to execute an attack and encrypt all files it has access to). On reused runners, data might be persisted on disk as well, leading to attacks like the following:

* *Cache poisoning*—This may take the form of overwriting `node_modules` at the runner level, for example. The next job will use the dependency from the cache. This applies for any package manager’s local caching system. An attacker can even prep your local Docker images with their own version, by mislabeling their version of a Docker image with a label you are using.

* C*hanging environment variables*—This includes changing other things, like SSH keys and configuration files for your package managers, including .npmrc, .bashrc, and others. This could be misused to let the package manager search for all packages on an endpoint controlled by an attacker, instead of using the default package managers URL.

* *Overwriting tools in the /opt/hostedtoolcache/ directory*—This is the default storage for actions like setup-node, setup-java, and setup-go.

* *Credential hijacking by retrieving the credentials used to register the runner*—These credentials are always stored in the runner folder itself, which means they are also accessible from inside a job. In section 6.3.2, you will find a way to mitigate the risk of using these credentials to spin up a new runner in a different location.

As a best practice, avoid running a job on a self-hosted runner without having full control over the job definition. Especially with public repos hosted on https://github.com, where any authenticated user can craft a pull request to attack your setup, we cannot stress enough that you should never run a job on your self-hosted runner with access to your private network. GitHub protects you from these types of attacks by limiting the `GITHUB_TOKEN` for the `on: pull_request` trigger and allowing you to choose the level of manual approval that will be required to run workflows on incoming pull requests from new contributors, as shown in figure 6.11.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH06_F11_Kaufmann.png)<br>
**Figure 6.11 Settings for running workflows from outside collaborators**

If you still need to run a job on your self-hosted runner, then run it on a contained runner that is ephemeral (single use), does not have any networking connection options, and is only allowed to run after running stringent security checks, both manual and automated. You can, for example, run specific linters for GitHub Actions on your workflows to detect things like shell-injection attacks (running injected code from `run` commands). One of those linters is the `ActionLinter` (https://github.com/devops-actions/actionlint), which will check for shell-injection attacks based on untrusted user input, like, for example, the title of an issue, the name of a branch, or the body of a pull request.

Another way to protect your workflows, and thus self-hosted runners, is to have environment protection rules that allow a job to only run when, for example, (manual) approval is given or when custom checks (environment protection rules) have completed successfully. You can even configure an environment to only allow jobs to run when they come from a certain branch. In figure 6.12, you can find an example where a custom protection rule has been configured by using a GitHub app that will run the checks. Additionally, GitHub already blocks workflows from running when coming from a fork or from a new contributor to the repository.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH06_F12_Kaufmann.png)<br>
**Figure 6.12 Environment protection rules**

## 6.3 Single-use runners

There are three runtime options for setting up self-hosted runners:

* *Environments that are continuously available to run new jobs (running as a service)*—That means that the same machine is always ready to handle a queued job.
* *Ephemeral runners that only are available for executing a single job*—These shut down when that job is completed.
* *Ephemeral runners with JIT tokens*—These are only available for a single job, and the token to register the runner can only be used once.

Our recommendation is to use ephemeral runners with JIT tokens whenever possible because of the security concerns of persisting data from job 1 that then can be (mis)used in job 2 on the same runner. GitHub-hosted runners are configured the same way, to protect data being leaked between customers. With this setup, you also get a fresh runner with every job, so there is less chance of becoming dependent on a specific runner that has some files cached or software preinstalled. You are now required to specify all the tools you need to execute in your job definition. This significantly increases the portability of your workloads as well.

### 6.3.1 Ephemeral runners

You configure an ephemeral runner by adding the `--ephemeral` parameter to the runner configuration script. This will put the runner online, waiting for a job to run. When a single job has been executed, the runner will deregister itself and stop running. Not a single extra job will land on that runner. Be aware that the environment for the runner itself will still linger around, depending on the solution. For example, if you install this ephemeral runner on a virtual machine (VM), the VM will still be up and running, even though the runner itself deregistered from the GitHub environment and stopped itself from running. You can use the `ACTIONS_RUNNER_HOOK_JOB_COMPLETED` hook to handle the completion of the job and, for example, clean up the VM (and spin up a new VM to handle new incoming jobs the same way).

### 6.3.2 Just-in-time runners

The token that is used to register self-hosted runners is always valid for an hour and is stored on the runner itself and available from inside a job. That makes it possible to steal these credentials and start a new runner with the same credentials in a different location. If you want to make this setup more secure by limiting the exposure of that credential, then you can use *just-in-time* (JIT) runner configuration. JIT runners work the same as with the ephemeral setup: the validity duration of the installation token is the only difference (one hour vs. one time usage).

To get the configuration needed to register a new runner with the JIT configuration, you need to make an API call to the following endpoint (shown in listing 6.9): `/orgs/{org}/actions/runners/generate-jitconfig`. The response can be used in the script to start up the runner. Instead of `--ephemeral`, you call the script as follows: `./run.sh --jitconfig ${encoded_jit_config}`. The encoded JIT configuration value is only valid for one installation of a self-hosted runner, and it cannot be reused.

The new JIT runner will only accept one single job execution. On completion of that job, it will automatically be removed from the enterprise, organization, or repository level for which it was created and the service will stop running. It is still your responsibility to clean up the runner and prevent reuse of the same environment. For that, you can use the `ACTIONS_RUNNER_HOOK_JOB_COMPLETED` hook to handle the completion of the job.

Listing 6.9 Creating a JIT runner

```sh
curl --location 'https://api.github.com/orgs/GitHubActionsInAction/actions/runners/generate-jitconfig' \
--header 'X-GitHub-Api-Version: 2022-11-28' \
--header 'Content-Type: application/json' \
--header 'Authorization: Basic <encrypted token>' \
--data '{
    "name": "New JIT runner",
    "runner_group_id": 1,
    "labels": ["jitconfig"]
}'
```

## 6.4 Disabling self-hosted runner creation

Keep in mind that, by default, every user with admin-level access (enterprise, organization, or repository level) can get to the self-hosted runner screen and start installing a runner in their environment. To control this, it is possible to disable the creation of self-hosted runners at the enterprise or organization level. In figure 6.13, you can see the options you have at the organization level. This gives you more control over where a self-hosted runner can be created. At the organization level, you can either allow for all repositories, disable it for all repositories, or enable the creation for specific repositories.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH06_F13_Kaufmann.png)<br>
**Figure 6.13 Disabling self-hosted runners at the organization level**

On the enterprise level, you can completely disable the creation of self-hosted runners for all organizations. The user interface for this can be seen in figure 6.14. If you have enterprise-managed user (EMU) organizations, then it is also possible to disable it for any repositories in the `personal` namespace that are in the user space for those organizations.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH06_F14_Kaufmann.png)<br>
**Figure 6.14 Disabling self-hosted runners at the enterprise level**

After disabling the creation of self-hosted runners, users will get the warning shown in figure 6.15. Any runners that have been created before these settings were enabled will still be running and executing jobs. You will need to check the organizations where you disallowed self-hosted runners, and then remove the existing runners manually. Note that users can still create self-hosted runners for repositories created in their own user space.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH06_F15_Kaufmann.png)<br>
**Figure 6.15 Self-hosted runner creation disabled**

## 6.5 Autoscaling options

To set up runners in an automated way, we recommend looking at the curated list of solutions in this [repository][5]. There are options to host runners on Amazon EC2 instances, AWS Lambda, Kubernetes clusters, OpenShift, and Azure VMs—and you can, of course, set up an Azure scale set yourself as well. Some of the solutions will scale for you by themselves, by using GitHub API endpoints to check for incoming jobs. Several solutions also support rules that let you scale up or down based on time of day (e.g., scale up between business hours and down outside of business hours) or scale up and down based on the number or percentage of runners executing a job at a given moment.

It’s also possible to scale with a webhook in a GitHub app on the event `workflow_job`. This webhook is triggered every time a job is queued, waiting, in progress, or completed. These events let you trigger the creation and deletion of a runner with, for example, the corresponding labels for that job. Using this webhook gives you full control over the runners that are available, including where to create them (e.g., in the correct network) or which hardware capabilities the runner will get. Setting up a webhook can be done at the organization or enterprise level, as shown in figure 6.16.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH06_F16_Kaufmann.png)<br>
**Figure 6.16 Scaling webhook setup**

### 6.5.1 Autoscaling with Actions Runner Controller

The *Actions Runner Controller (ARC)* solution is owned by GitHub and gives you an option to host a scalable runner setup inside your own Kubernetes cluster (a setup where multiple computers share the workload and scheduling is handled for you). If you have the option to host your own Kubernetes cluster for this and be in control how the cluster is utilized and scales, then we recommend this solution over others.

Note that ARC only supports Linux-based Kubernetes nodes, so there is no option to run with Windows-based nodes in your cluster. With ARC, you get control over the Docker image that is executed, so you can configure extra tools that are preinstalled by adding it to the container you configure. You also have control over the available hardware resources that the runner has by configuring the resource limits on the pod deployments. Since you manage and maintain the Kubernetes cluster, you also have control over what the runners can connect to, so you can really limit access to the internet, for example—something that some enterprises do require. ARC runners are set up as ephemeral runners by default; the container will execute one job and exit the container. As it uses Kubernetes replica sets, Kubernetes will spin up a new container automatically.

With the ARC solution, you have the following options:

* Scale up and down based on a schedule
* Scale based on the percentage of runners that are busy executing a job (and then scale up or down by a configurable number of runners)
* Spin up new runners on demand, by listening to the API webhook that GitHub will trigger when a new pull request is created

The ARC solution supports creating runners at the enterprise, organization, and the repository levels, giving you the most flexibility in creating shared runners. You can also configure a scale set for Team A and another one (with different scaling rules and a different even container image!) for Team B. By using a Helm chart to configure the scale set, you can let the configuration land in different Kubernetes namespaces to give you more separation between them as well as options for networking and limiting access across namespaces.

Note that the ARC solution will spin up ephemeral runners, so any caching you want to do in the runners will have to be done inside the container images you use or rely on—for example, Kubernetes to cache the Docker containers you use. The images can be spun up using a rootless setup, making it a lot more secure (as breaking out of the container is harder when using rootless).

### 6.5.2 Communication in ARC

ARC lets you configure the communication with either a personal access token (PAT) or a GitHub app. Since there is no GitHub app support to configure runners at the enterprise level, a PAT is required there. For all the other levels (organization or repository), we recommend using a GitHub app—that way, you are not tied to a single user account and can really set up a fine-grained app that can only be used to register runners and nothing else. Using a PAT is discouraged, as the PAT can impersonate everything the user can do instead of having fine-grained control over the available scopes of the token. Additionally, if the engineer whose token is used were to leave the company, thus invalidating the PAT, you would be left with a broken setup for which it will take some time to figure out what happened. GitHub apps also do not take up a license seat, saving on those costs as well.

With a GitHub app, you’ll get an installation ID and a private key file (a PEM file) that can be given to the ARC controller as a Kubernetes secret, which will be used to register and deregister runners. You can also use the GitHub app as the receiving end for the webhooks available in GitHub to trigger a runner to be created whenever a job is queued. Each time a new runner is requested, the GitHub app information will be used to get a fresh installation token from the GitHub API, and then the runner will be registered with that token.

### 6.5.3 ARC monitoring

There is very little monitoring for action runners in general, as you will see in chapter 7. The only user interface available is the one that shows you which runners are available at each level and if they are busy. Even the available APIs only show that information: which runners are available, whether they are busy, and which labels are assigned. There is also no method to get that information out of ARC, like getting a count of available runners for a certain label or getting information about the percentage of runners that are busy at the moment. For monitoring purposes, you will need to set something up yourself. You can use Kubernetes monitoring to check how many pods are up and running and link that with dynamic scaling settings to see how you are doing. Then, you can configure alerts if you are scaling up or down relatively fast. Alternatively, you can create a workflow and use an action from the [marketplace][6], and then you can use that to get all the information on available runners and determine the number of runners available for a certain label (and alert if the count is lower than expected) or check how many runners are busy executing a job.

## Summary

* This chapter covered self-hosted runners and when to use them, as well as security risks and the different setup options you have available.

* Self-hosted runners can be configured in any environment that supports the .NET core, Git, and node.

* Installing Docker is optional, but it’s necessary to run actions that are based on a Docker image.

* The self-hosted runner communicates with an outbound HTTPS connection, which makes installation in your network easier and more secure.

* You have a lot of runner configuration options, allowing you to customize what happens before and after a job.

* The best way to set up a runner is by configuring it as ephemeral. Then, it will only run a single job and then deregister itself, not accepting any more jobs. That gives you the option to clean up the environment and prevent significant security risks.

* There are several autoscaling options available; the one that is managed and supported by GitHub is the Actions Runner Controller. This can scale based on time, runner utilization, and just in time by configuring a webhook in GitHub that triggers whenever a workflow job is queued.

[1]: https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners/about-self-hosted-runners#supported-architectures-and-operating-systems-for-self-hosted-runners
[2]: https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners/about-self-hosted-runners#communication-between-self-hosted-runners-and-github
[3]: https://github.com/actions/runner-images/blob/ubuntu22/20230611.1/docs/create-image-and-azure-resources.md
[4]: https://github.com/actions/runner-container-hooks
[5]: https://github.com/jonico/awesome-runners
[6]: https://github.com/devops-actions/load-runner-info