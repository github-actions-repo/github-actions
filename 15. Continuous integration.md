- [Continuous integration](#continuous-integration)
  - [8.1 GloboTicket: A sample application](#81-globoticket-a-sample-application)
  - [8.2 Why continuous integration?](#82-why-continuous-integration)
  - [8.3 Types of CI](#83-types-of-ci)
    - [8.3.1 Using a branching strategy: GitHub Flow](#831-using-a-branching-strategy-github-flow)
    - [8.3.2 CI for integration](#832-ci-for-integration)
    - [8.3.3 CI for quality control](#833-ci-for-quality-control)
    - [8.3.4 CI for security testing](#834-ci-for-security-testing)
    - [8.3.5 CI for packaging](#835-ci-for-packaging)
  - [8.4 Generic CI workflow steps](#84-generic-ci-workflow-steps)
    - [8.4.1 Getting the sources](#841-getting-the-sources)
    - [8.4.2 Building the sources into artifacts](#842-building-the-sources-into-artifacts)
    - [8.4.3 Testing the artifacts](#843-testing-the-artifacts)
    - [8.4.4 Test result reporting](#844-test-result-reporting)
    - [8.4.5 Using containers for jobs](#845-using-containers-for-jobs)
    - [8.4.6 Multiple workflows vs. multiple jobs: Which to choose?](#846-multiple-workflows-vs-multiple-jobs-which-to-choose)
    - [8.4.7 Parallel execution of jobs](#847-parallel-execution-of-jobs)
  - [8.5 Preparing for deployment](#85-preparing-for-deployment)
    - [8.5.1 Traceability of source to artifacts](#851-traceability-of-source-to-artifacts)
    - [8.5.2 Ensuring delivery integrity: The software bill of materials](#852-ensuring-delivery-integrity-the-software-bill-of-materials)
    - [8.5.3 Versioning](#853-versioning)
      - [Semantic versioning](#semantic-versioning)
      - [Calendar versioning](#calendar-versioning)
    - [8.5.4 Testing for security with container scanning](#854-testing-for-security-with-container-scanning)
    - [8.5.5 Using GitHub package management and container registry](#855-using-github-package-management-and-container-registry)
    - [8.5.6 Using the upload/download capability to store artifacts](#856-using-the-uploaddownload-capability-to-store-artifacts)
    - [8.5.7 Preparing deployment artifacts](#857-preparing-deployment-artifacts)
    - [8.5.8 Creating a release](#858-creating-a-release)
  - [8.6 The CI workflows for GloboTicket](#86-the-ci-workflows-for-globoticket)
    - [8.6.1 The integration CI for APIs and frontends](#861-the-integration-ci-for-apis-and-frontends)
    - [8.6.2 CI workflows for quality control](#862-ci-workflows-for-quality-control)
    - [8.6.3 The CI workflow for security testing](#863-the-ci-workflow-for-security-testing)
    - [8.6.4 The CI workflows for container image creation and publishing](#864-the-ci-workflows-for-container-image-creation-and-publishing)
    - [8.6.5 Creating a release](#865-creating-a-release)
  - [8.7 Conclusion](#87-conclusion)
  - [Summary](#summary)

# Continuous integration

This chapter covers

* Achieving fast feedback with continuous integration
* Differentiating between integration workflows
* Defining continuous integration workflows
* Ensuring the integrity of artifacts
* Creating a release for your continuous deployment workflows
* Setting up a continuous integration workflow

*Continuous integration* (CI) is a DevOps practice, in which you regularly merge code changes into the central repository and run automated builds and tests to check the correctness and quality of the code. CI aims to provide rapid feedback and identify and correct defects as soon as possible. CI relies on the source code version control system to trigger builds and tests at every commit.

CI is composed of a set of steps that delivers the output artifacts we need to run a system in production. Which set of steps are required depends on the programming language and tools you are using as well as the platforms you are targeting with your product. In this chapter, we will lay out the generic set of steps each CI process typically entails and how you can set up GitHub actions to trigger on each commit and deliver this as a set of artifacts that can be picked up in a subsequent process of *continuous deployment* (CD), where you deploy the product to preferably a production environment (covered in chapter 9).

## 8.1 GloboTicket: A sample application

The following paragraphs will guide you on how you can build an application, before covering how we will deploy this application to production in chapter 9. To give some real-world examples of how you can create a CI and CD workflow, we will use an application written in C# and deployed to the Azure Cloud. We picked a solution that can be deployed to a Kubernetes cluster, since that is very common these days. Remember, the application is used to illustrate the concepts, and all steps and concepts we use to build and deploy an application to the cloud are applicable to any piece of software you have. Using GitHub actions, you can deploy any application to any infrastructure. The architectural diagram for the application is visualized in figure 8.1. You see, we have a web application that shows the frontend of the application, which is a web app. The application uses two APIs: one to retrieve the tickets that can be sold (cataloging) and another to register the orders that have been placed (ordering).

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH08_F01_Kaufmann.png)<br>
**Figure 8.1 The GloboTicket architecture**

The moment you deploy this application, you should see a website that shows you tickets you can buy to attend a concert. The deployed application is shown in figure 8.2.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH08_F02_Kaufmann.png)<br>
**Figure 8.2 The GloboTicket home page**

You can find the sample application yourself on [GitHub][1]. The application is based on a microservices architecture and requires three containers to be deployed to a Kubernetes cluster.

## 8.2 Why continuous integration?

The first mention of *continuous integration* dates to 1989, during a computer software and applications conference in Orlando, where Gail Kaiser and colleagues introduced the topic in their panel, “Infuse: Fusing Integration Test Management with Change Management” ([International Computer Software & Applications Conference][2]. In the ’90s, software methodologies like extreme programming also experimented with this concept. It really picked up popularity in the early 2000s, just after the “[Manifesto for Agile Software Development][3]” had gained traction. The “Agile Manifesto” is based on 12 principles, the first of which states, “[Our highest priority is to satisfy the customer through early and continuous delivery of valuable software][4]”. To get into the state of continuous delivery, we first need to ensure our codebase is always in a so-called “buildable state.” In the past, we built our codebase infrequently and had to take a significant amount of time to integrate changes from many team members, who committed changes over a significant period of time. When you implement CI, you spend way less time on integrating the software with the changes of others, which ultimately reduces the waste that is spent in resolving code change conflicts. So the bottom line is that we use CI to reduce waste in our software-delivery process by integrating the software to the central repository at each commit, instead of spending a lot of time fixing all integrations that would otherwise accumulate over time.

## 8.3 Types of CI

Because we strive to integrate the software as soon as possible on each commit to the central repository, we also distinguish different types of CI. Each of these types of CI strives for different goals as part of the final goal of moving software to production as quickly as possible. We can run these different types of CI in parallel with each other to produce results much faster and give the developer feedback as soon as possible. Even across different companies, the authors of this book have noticed a common pattern in categories of workflows. First, you have the category that has the goal to create feedback on the integration as soon as possible. This is the primary reason for CI. The workflow needs to be as fast as possible. The second category has various goals that differ from this fast feedback CI. This can be the creation of packages for the final delivery of the software, reporting on the quality of the software, providing insights in security, and so on. Because they have a different purpose, you can trigger them less frequently and can have slower response times for your feedback to the developers. This is represented in figure 8.3.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH08_F03_Kaufmann.png)<br>
**Figure 8.3 Different types of CI**

Let’s have a deeper look at when we would trigger each type of workflow and what the goal of each type actually is. For this, we will refer to the way we work with our code repository with a specific branch strategy. We have chosen to use the most popular strategy here: GitHub Flow.

### 8.3.1 Using a branching strategy: GitHub Flow

*GitHub Flow* is the advertised way of working when you use Git and deliver a software product the DevOps way. With GitHub Flow, you create a branch called feature/name-of-feature, and you commit your changes to that branch. When you think your feature is complete, you open a pull request where you solicit feedback and have the peer review of your code. After some discussion and final approval from your team members, the pull request is accepted and the change is merged into the main branch before being deployed to production. This workflow is visualized in figure 8.4.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH08_F04_Kaufmann.png)<br>
**Figure 8.4 The GitHub Flow branching strategy**

We can set up GitHub in such a way that you protect the main branch from any direct commits and every change needs to come from a pull request. This way of working is encouraged, since it provides a great way to control the quality of what goes into the main branch and you also enforce a *four-eyes principle*, which is required by most compliance frameworks. This principle ensures there is, at minimum, one additional person involved in moving a change to production. It is also known as the *rule of segregation of duties* and is a risk mitigation control. Using this way of working also enables you to comply with regulations in industries with heavy governance. Enforcing this flow also enables you to set up action workflows that can receive very fast feedback on the work you do on the feature branch, and it ensures there is always a stable main branch in a deployable state. In the remainder of this chapter, we describe different types of CI you can distinguish between and how applying the various action workflows to particular steps in the GitHub Flow process can save you a lot of time and compute resources. We employ GitHub Flow as our branching strategy for the remainder of the book, since it is the most common way of working nowadays. This way, all the examples we provide can be used immediately without many modifications.

### 8.3.2 CI for integration

This CI process strives for validation if the software you just committed to the repo can be integrated into the source code. This entails compiling the code to the type of artifact you need for production. The integration CI strives to provide results as quickly as possible, and you should strive for swift feedback to the developer. If failures occur, this implies the code is not integrated, and the developer typically takes action immediately to fix the integration problems. Errors that occur here are often compiler errors, warnings, and the like. This workflow is normally triggered on the feature branch in GitHub Flow. The workflow on the main branch often entails more steps, to ensure complete validation of all we need before deployment.

### 8.3.3 CI for quality control

This process validates the quality of the source code that was committed. This involves simple quality control checks, like linting the source code for readability, checking if the code has multiple duplications of the code, and ensuring the code has passed a set of maintainability metrics. There are also some more involved quality control checks, like validating the code is written securely and it delivers the functionality, based on automated tests. In this process, you can include a variety of tools that will give you insights into the quality of the code currently in your source code repository. Tools you can think of that are typically part of a CI build for quality control include linting tools, which check the syntax of the code against a set of rules; code metric tools, like SonarQube, which provide insights into maintainability and other code smells; unit testing tools, which validate the overall functional workings of the code base; and security tools, which can determine whether your code might be vulnerable to all kinds of known ways to attack software. Typically, these builds take longer to complete and often result in work that is placed back on the backlog to fix in a later stage of the development process. This workflow is often triggered when you create a pull request, so it provides input for the reviewers and helps ensure quality in the main branch.

### 8.3.4 CI for security testing

This process checks whether the software that is written is secure by default, using tools called *static application security testing* (SAST) and *dynamic application security testing tools* (DAST). GitHub itself also provides these tools as part of the tool suite, and they are fully integrated with GitHub Actions and the user interface on the web. When you have GitHub Enterprise, you can buy the rights to use the tools as an add-on capability. This product is called Advanced Security, and it is free for public repositories. With Advanced Security, you can create a security testing workflow that does advanced scanning of the software on known vulnerabilities that might have been introduced in your own software. You can also, of course, use any other tools you can find in the market that can help you do security scanning on your software. Well-known vendors here are Snyk, Black Duck, and Mend. This CI type is also triggered at the moment of a pull request, to ensure we don’t bring new security vulnerabilities to the main branch. It also should be part of a regular schedule on the main branch, since new vulnerabilities emerge in the software ecosystem without us needing to change our code. Having this on a schedule on the main branch ensures we always know the potential security problems we ship to production. We also can decide to block releasing the software as part of this action workflow to ensure vulnerabilities of a certain severity level are always mitigated before release.

### 8.3.5 CI for packaging

This process aims to produce the final artifacts to deliver the software to production. Here, we can target multiple platforms and create builds that are optimized for production purposes. While previous builds can, for example, still include debug type of builds, these builds provide the clean artifacts we move to production. Removing debug information is often forgotten and can, aside from creating a larger-sized artifact, create a potential security risk. The end result of this build is that we get the final artifacts delivered to either an artifact store (e.g., the package management store), delivered to the container registry, or uploaded to GitHub Actions storage so that it can be picked up at a later time by the CD Actions workflow.

## 8.4 Generic CI workflow steps

Every CI workflow has the same set of generic steps:

1. Get the sources.
2. Build sources into artifacts and perform some very quick initial checks.
3. Publish the results.

Let’s have a look at these steps and see how we can optimize them for each type of CI.

### 8.4.1 Getting the sources

You get the sources from your repo with the action `actions/checkout`. The action to get sources can also be tuned to what you actually need to get from the source repository. To speed up your workflows, it often makes a lot of sense not to fully clone the repository but get only the tip of the main branch. This can speed up the operation significantly, especially for repositories with a longer lifespan. You can also control which branches are retrieved and the depth of the repository you clone. The following listing provides an example, where we only retrieve the tip of the main branch, since this is often the only data you need to see (e.g., if the source code compiles and integrates with what is in the current repository).

**Listing 8.1 The actions/checkout action**

```yml
- uses: actions/checkout@v3
  with:
    ref: 'main' #not naming the ref will fetch the default branch
    fetch-depth: '1' #1 is default and 0 fetches the full depth of the repo
```

If you want to get the repository with the full history, you can set the property fetch-depth to 0, this will get you the full history of the repo. This is only needed when you are going to traverse the history of the repo as part of the next steps in your CI. Sometimes, other actions need this, so it is good to know it is possible with only a simple change. The default for `fetch-depth` is `1`.

### 8.4.2 Building the sources into artifacts

Once you have the sources available, you can take steps to build the source into artifacts that you need to validate if the software is doing what it is supposed to do. A common practice is to compile the sources into binary files or create container images that can be used to deploy the application.

We use the sample application to give you a concrete example of the next step in your workflow. This application first needs to be compiled, and then we run the basic unit tests to validate the basic behavior of the application, after which we create container images that can be used for deployment.

Since the sample application is a .NET core application, the step to compile the sources requires using a tool called the dotnet command line interface. Some tools are already installed on the GitHub hosted action runners; the dotnet tooling is a good example of this. To get a full list of the tools installed on the runners, you can view the [documentation][5], as described in chapter 6. The following listing shows how to compile the .NET code into binaries.

**Listing 8.2 Compiling the .NET core code**

```yml
    - name: Setup .NET
      uses: actions/setup-dotnet@v3
      with:
        dotnet-version: 6.0.x
  
    - name: Restore dependencies
      run: dotnet restore
  
    - name: Build
      run: dotnet build --no-restore
```

The code example in listing 8.2 only builds the code. The sample application will eventually run on a Kubernetes infrastructure, so we must create a container image. Now, we can make a choice here always to build a container image, but this would significantly slow down the workflow compared to a simple build of the C# files in the project.

If we go back to the GitHub flow approach of branching, we can also divert this and make it part of the processing of the pull request. That way, you can suffice with only quick feedback if the sources are in good shape. The workflow you need has a different purpose: to produce the required artifacts for us to deploy to an environment and validate if the code adheres to coding standards, license checks, and so on, before it is accepted into the main branch. This is not necessary for your feature branches, only when you merge to the main branch. You can trigger the next workflow the moment you create the pull request.

> [!NOTE]
> 
> You might think this is a bad idea if your compile step takes a few hours because you are building a large code base. When Actions started, this was true, but nowadays, we have a caching option, where we can decide what the cache key will be. When you share the key between jobs, you can use the cached artifacts and nicely separate the concerns of CI and the steps to create the final output for delivery. More on caching can be found in chapter 12.

### 8.4.3 Testing the artifacts

The tests we run during CI for validation only involve tests that can show if the integration of the sources is successful. Preferably, this would only involve the tests that can verify the effect of the change. Often, this is not easy to determine, and the most common tests you run in this step are the unit tests that are part of the sources you are building. In our case, this is .NET, and we can use the dotnet command line to kick off the tests. The result of the test run should indicate success or failure, which we can use in our other steps in the workflow as an indicator if we should continue our run. Any test tool you use can set the workflow state to failure by producing an exit status code other than 0.

For example, you can run the unit tests that are part of the sample application by running the dotnet command line dotnet test. It will produce an exit code greater than 0, indicating the number of tests that have failed. If all tests pass, the command line will return 0, indicating success.

**Listing 8.3 Using the command line to run tests**

```yml
    - name: Test
      run: dotnet test --no-build --verbosity normal
```

### 8.4.4 Test result reporting

By default, GitHub has no way to output test results other than the console’s built-in reporting. But often, especially when tests fail, you’d like to see a report of which test failed and which was successful. You can still get the data in the final workflow report by adding information to the job summary description. This is done by outputting data to the output variable available in your workflow run. This is called `$GITHUB_STEP_SUMMARY`.

You get things in the result summary by pushing any text in Markdown format. This is then rendered in the output report of the job. In the following listing, you can see an example of outputting text to this variable and the results output that will be reported on the job results page.

**Listing 8.4 Using $GITHUB_STEP_SUMMARY to visualize test result output**

```yml
  name: "chapter 08: Generate job output using markdown"
on:
  workflow_dispatch: 

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Generate markdown
        run: |
          echo "## Test results" >> "$GITHUB_STEP_SUMMARY"
          echo "| **Test Name** | **Result**|" >> "$GITHUB_STEP_SUMMARY"
          echo "|--|--|" >> "$GITHUB_STEP_SUMMARY"
          echo "| validate numbers are > 0 |:white_check_mark: |" >> "$GITHUB_STEP_SUMMARY"
          echo "| validate numbers are < 10 |:white_check_mark: |" >> "$GITHUB_STEP_SUMMARY"
          echo "| validate numbers are odd |:x: |" >> "$GITHUB_STEP_SUMMARY"
          echo "| validate numbers are even|:white_check_mark: |" >> "$GITHUB_STEP_SUMMARY"
```

This will result in a summary that contains a nicely formatted table with the results. An example is shown in figure 8.5.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH08_F05_Kaufmann.png)<br>
**Figure 8.5 Summary results in Markdown**

Various test tools are available for different ecosystems. Some of them produce markdown reports that you can integrate with the workflow by utilizing the GitHub step summary and pushing the data of such a file to this output. At the end of this chapter, we will show you how this is done with a concrete example of using a Markdown logger that can be integrated with the `dotnet` tools.

### 8.4.5 Using containers for jobs

When we showed how to build sources into deployable artifacts, we could also have picked an alternative way to run those tools. Instead of relying on tools available on the GitHub runners or installing it during the run, you can also pick a container image yourself that you use to run your tools. You can select any image from the Docker hub, refer to your container registry, and retrieve your container image to run a build. You only need to specify the container and set up a mounted volume to point to the sources you get from GitHub and where you place the produced output. Using container images versus relying on the available tools on the runners is a matter of preference. This can also sometimes be a way to circumvent problems with licensing of tools that require elaborate setup and setting licensing keys. Another advantage may be that you can also run the build locally with a Docker command and don’t need a separate setup for local work. You also have full control over the history. Even 10 years later, you are able to build the sources again as long as you don’t delete the image.

> [!NOTE]
> 
> Since the container image you want to use needs to be downloaded, you can incur some slowdown at the start of the workflow.

You can also use images that come from a private repository. To do this, you need to provide the location and credentials to pull the image from the runner. Listing 8.5 shows the exact same steps for building the `dotnet` application, but now, it runs from a container image hosted on Docker hub. You can execute actions like you are used to in any action workflow, but the big difference is that you don’t need to set up all kinds of tools and configurations—you can start directly with the task at hand.

As an example, you can see the exact same workflow in listing 8.5 as was previously shown to build the frontend of the GloboTicket application using the `dotnet` tools. Using the available Microsoft SDK container image saves you from installing and configuring the right tools, and you can start the build process immediately.

**Listing 8.5 Using a container for jobs**

```yml
name: Build inside container
on:
  push:
    branches: [ main ]
  workflow_dispatch:
jobs:
  container-build-job:
    runs-on: ubuntu-latest
    container:
      image: mcr.microsoft.com/dotnet/sdk:6.0
    steps:
      - uses: actions/checkout@v3
      - name: Build
        run: dotnet build 
      - name: Test
        run: dotnet test --no-build --verbosity normal
```

### 8.4.6 Multiple workflows vs. multiple jobs: Which to choose?

An action workflow always has one or more jobs. These jobs are run in parallel and can have dependencies with other jobs. When a job is dependent on another job, these jobs are executed in sequence. A workflow is contained in one YAML file, and you can have more jobs in one file. You can create multiple workflows for a repository.

So when should you choose a new workflow or a new job to do some work? In most examples you see on the web as well as those offered by GitHub itself, you often see multiple jobs in one file. While this is convenient in terms of keeping everything in one place, it also creates some problems. The main one is determining who will maintain the workflow file and who gets to review this file before the change is accepted. Especially in highly regulated organizations, there needs to be a strict separation of duties when making changes that can affect the deployment to a production environment.

Another question is, *What will change when I alter my software?* When you make changes to the source code and its dependencies, this should only affect a small part of the system, not everything you have in terms of automation.

You can even boil this down to a very commonly used term in software development and one part of the *SOLID* Principles. The S in the acronym SOLID stands for *single responsibility*, and we use this to keep changes to a minimum and make maintenance less difficult and brittle over time. If you keep the work that needs to be done simple and have clearly defined reasons to run a particular workflow with specific goals, you will end up with some more workflow files that all have a single job. When you combine this with a well-defined branching strategy, you can very nicely use the different event types that we have in the development cycle as the moments you want to trigger a particular piece of automation.

When we come to CD, we often need to run automation on different machines. A job also has the ability to run on another machine. For this reason, it makes total sense to have multiple jobs, since each job can then execute on another machine. In this case, the jobs are a means to distribute the work, but the type of work is exactly the same, as we will see in the next chapter. Based on this, we propose a set of small workflow files with a specific purpose or goal. This keeps the cognitive load during maintenance on those files low and the group of people who need to review it specific from an audit perspective. Please treat this as guidance, not a must-follow rule. If there is a reason to use multiple jobs, then please do so.

### 8.4.7 Parallel execution of jobs

In some situations, you might want to run a set of jobs in parallel that do the same thing but with a few other parameters. An example of this would be building artifacts for various platforms, like ARM and x86. For this, we can use the concept of matrix job strategy. With the matrix strategy, you can use different variables to build the same code in a single job definition for different platforms and tooling. In the following listing, you can find an example of a matrix strategy that builds the code on two platforms with three different node versions.

**Listing 8.6 Using the matrix strategy for executing jobs in parallel**

```yml
jobs:
  build:
    strategy:
      matrix:
        dotnet-version: [6, 7]
        processor: [x86, arm]
```

In this example, we would start parallel jobs for the following builds:

* dotnet version 6, processor x86
* dotnet version 6, processor ARM
* dotnet version 7, processor ARM
* dotnet version 7, processor x86

## 8.5 Preparing for deployment

In your CI workflow that creates the final artifacts, you need to define where you want to store them for the next phase in the process: the CD phase. There are a few things that are important when we are preparing for release. We want to ensure we can trace back which change was made, by whom it was made, and how we can track it back from the environment we deployed to. We also want to ensure we use proper version numbering, and we want to ensure you can deploy the created artifacts in the most convenient way to various environments.

Let’s dive a bit deeper into traceability, versioning, and creating a GitHub release. After we create a release, we use GitHub package management to store our artifacts in GitHub package management or the GitHub container registry.

### 8.5.1 Traceability of source to artifacts

When you work in more compliance-heavy organizations, you need to be able to prove a certain change in the source code is tied to a requirement and that this particular source change is deployed in an environment. With GitHub, you can make use of the fact that not only the actions are integrated with your source repository, but GitHub also provides ways to track requirements, defects, feature requests, and more. This is all done with the use of GitHub issues.

When you commit source code to the repository, you can, for example, enforce the code to be validated before it is committed. You would use pull requests to achieve this, and you can enforce that they are used by setting a branch policy. In your guidelines for approving a pull request, you can check that at least one issue is attached to the pull request, so there is a traceable history to the requirement that was implemented with the code change. Unfortunately, branch policies don’t have a way to enforce the required traceability to issues. So you need to have the reviewer check themselves or create an action yourself to do this verification. Setting the branch policy is crucial here. You can set branch policies using the settings page, as shown in figure 8.6.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH08_F06_Kaufmann.png)<br>
**Figure 8.6 Branch protection rules**

When you use a pull request to merge the changes into the main branch, you can ensure there is always traceability to the requirements as well as a four-eyes principle in place, which is a requirement in almost any governance framework for your compliance. With these comments in the commit messages, it is now possible to track any change back to a requirement or change request defined as an issue. Figure 8.7 shows how you can refer to an issue in a pull request, to be sure to trace the changes back to the requirement.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH08_F07_Kaufmann.png)<br>
**Figure 8.7 Tracing back to requirements**

### 8.5.2 Ensuring delivery integrity: The software bill of materials

Following the attack on [SolarWinds][6], our industry became more aware of a new kind of customer vulnerability: attacks via the CI/CD infrastructure we use as developers. This has imposed a new burden on us to validate if the software we created is actually the software we expected to deliver and as well as if everything we used during the creation of the software was not tampered with. In May 2021, the president of the United States even signed an executive order requiring software companies to help improve the nation’s cyber security, including a way to validate the integrity of the software in production ([Executive Order 14028][7]). When we are running workflows that create artifacts, we want to ensure the integrity of those artifacts to prevent them from being tampered with during or after creation. We can go about confirming our artifacts’ integrity by following a multistep process, including validating the individual files used during creation as well as confirming the tools we used are not compromised.

This requires multiple layers of validation for each workflow. We need to check which actions we used with a workflow as well as where the files we used came from. Also, when we produce an artifact we will use in our delivery workflow later, we need a way to transfer those files securely and easily. In section 8.5.6, we will go into more detail about where you can store the artifacts before you start the deployment.

We also need to ensure this list of artifacts we use while running a workflow is not altered. For this, the industry has defined a set of standards to create a so-called bill of materials, known as a *software bill of materials* (SBOM). You can generate an SBOM in several ways. You can do this by retrieving it from the user interface or by making it during the creation of software artifacts in your action workflow. If you want an SBOM every time you create software artifacts, you are better off using a GitHub action and making this part of your standard workflow that prepares artifacts before deployment. With GitHub Actions, you can create an SBOM by using several actions that are available in the marketplace. Listing 8.7 shows how to use the Microsoft SBOM generator action that generates an SBOM that is compliant with the NTIA specifications and delivers this in software package data exchange (SPDX) format. This is the open standard for communicating software bill of material information. It is good to note there are two competing standards: CycloneDX and SPDX; Microsoft and GitHub have chosen to use the SPDX standard.

**Listing 8.7 Generating an SBOM using the Microsoft SBOM tool**

```yml
name: Generate SBOM
      run: |
        curl -Lo $RUNNER_TEMP/sbom-tool https://github.com/microsoft/sbom-tool/releases/latest/download/sbom-tool-linux-x64
        chmod +x $RUNNER_TEMP/sbom-tool
        $RUNNER_TEMP/sbom-tool generate -b ./buildOutput -bc . -pn Test -pv 1.0.0 -ps mycompany -nsb https://sbom.mycompany.com -V Verbose
```

Note that the example only shows you how to generate the SBOM. Normally, you also want to use this file as part of your release, and you should upload it to the release as an artifact that is part of the release.

### 8.5.3 Versioning

One important but commonly neglected element of releasing software is the versioning of what you release. There are many ways version numbers ae created in our industry, and in the last few years, you might have seen the industry moving toward more standardized versioning. Two of the most-used types of versioning are called semantic versioning and calendar versioning.

#### Semantic versioning

As the name implies, in *semantic versioning*, we adhere to a set of semantics when we bump a version number. The basic idea behind semantic versioning is that based on the version number, you can tell if a new version of a package, library, image, or artifact is backward compatible. The thinking behind this and all the details can be found in the documentation: https://semver.org.

In a nutshell, the versioning works as follows: given a version number MAJOR.MINOR.PATCH, increment a

1. MAJOR version when you make incompatible API changes
2. MINOR version when you add functionality in a backward-compatible manner
3. PATCH version when you make backward-compatible bug fixes

If you want the version number to be calculated based on your branches, you can use an action called *GitVersion* (see https://gitversion.net/), which is part of the `GitTools` action (see https://github.com/marketplace/actions/gittools). GitVersion looks at your Git history and works out the semantic version of the commit being built. For GitVersion to function properly, you have to perform a so-called un-shallow clone. You do this by adding the `fetch-depth` parameter to the `checkout` action and setting it to 0. Next, install GitVersion and run the `execute` action. Set an `id` if you want to get details of the semantic version, as shown in the following listing.

**Listing 8.8 Using the GitVersion action**

```yml
steps: 
- uses: actions/checkout@v3
  with: fetch-depth: 0
 
 - name: Install GitVersion
   uses: gittools/actions/gitversion/setup@v0.9.7
   with: 
     versionSpec: '5.x'
  
 - name: Determine Version
   id: gitversion
   uses: gittools/actions/gitversion/execute@v0.9.7
```

The calculated final semantic version number is stored as the environment variable `$GITVERSION_SEMVER`. You can use this, for example, as the input for the version of a package that you publish.

If you need to access details from GitVersion (e.g., major, minor, or patch), you can access them as output parameters of the `gitversion` task, as shown in the following listing.

**Listing 8.9 Using the version number by referring to the previous step**

```yml
 - name: Display GitVersion outputs
   run: | echo "Major: ${{ steps.gitversion.outputs.major }}"
```

With semantic versioning, it is also possible to indicate the quality of the build as part of the version number. You do this on prereleases or alpha versions of a soon-to-be-stable new version. It is common to use for this the notation: `v1.0.0-pre` or `v1.0.0-alpha`.

#### Calendar versioning

As this name implies, the version number is generated based on the calendar and the moment the workflow is executed. Depending on the release frequency of your application, you can choose to include the date up until the minute of release or simply keep it to today’s date. Listing 8.10 provides an example of how we can generate a calendar-based version. If we assume it is May 29th of 2023, then the output in the variable is `2023-05-29` and can be used in subsequent parts of the workflow by referencing the variable `$BUILD_VERSION`, using the environment context.

**Listing 8.10 Using the calendar action**

```yml
- name: Set Release Version
  run: echo "BUILD_VERSION=$(date --rfc-3339=date)" >> $GITHUB_ENV

- name: use the variable
  run: echo ${{ env.BUILD_VERSION }}
```

### 8.5.4 Testing for security with container scanning

In general, when you prepare artifacts to be deployed to a production environment, it is a best practice to ensure they are scanned for security. When building containers, we can use various tools to run a validation that searches for known vulnerabilities in the container image. I like to use the open source tool provided by aqua security, called Trivy. You can add Trivy scanning to your workflow by completing one additional step. The following listing shows how to use this action to scan your image and fail when it finds a vulnerability with the severity of `Critical` or `High`.

**Listing 8.11 Adding a container image scanning step**

```yml
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: '${{env.containerRegistry}}/${{env.imageRepository}}:${{github.run_number}}'
        format: 'table'
        severity: 'CRITICAL,HIGH'
        exit-code: '1'
```

By adding this extra step, your workflow will fail when a vulnerability is found in the container image, preventing you from pushing the image to the image registry. It is a best practice to always scan before you push your image to the registry so that a vulnerability never gets into an environment and causes a breach. Adding security as early as possible in the development cycle is often referred to as *shifting left*.

### 8.5.5 Using GitHub package management and container registry

Many organizations use artifact repositories to keep artifacts in a safe place, from which they can be pulled during the deployment phase. GitHub also offers an artifact repository, called GitHub Packages, which is available for multiple package management solutions. Table 8.1 lists the supported artifacts.

**Table 8.1 GitHub Packages supported artifacts**

| **Language** | **Description**                                        | **Package format**               | **Package client** |
|--------------|--------------------------------------------------------|----------------------------------|--------------------|
| JavaScript   | Node package manager                                   | package.json                     | npm                |
| Ruby         | RubyGems package manager                               | Gemfile                          | gem                |
| Java         | Apache Maven project management and comprehension tool | pom.xml                          | mvn                |
| Java         | Gradle build automation tool for Java                  | build.gradle or build.gradle.kts | gradle             |
| .NET         | NuGet package management for .NET                      | nupkg                            | dotnet CLI         |
| N/A          | Docker container management                            | Dockerfile                       | Docker             |

As the last step in your workflow, you can use the package manager that matches the ecosystem you are working on and push it to the GitHub Artifact Registry.

When building libraries, you publish packages that are used between projects or when you have a shared solution between various components or microservices. Packages are published and from there on used by other CI workflows. When you publish a package to an ecosystem like npm, NuGet, or RubyGems, it is a good practice to also create a release when you publish. This way it is clear you released a new version of your package, so others can pick it up. Creating a release is described in section 8.5.8, since it can also be a source to starting a deployment.

GitHub also provides a container registry where you can store container images you create during your CI workflows. To authenticate against the package management capability, we need to extend our authorization token to include `write` permissions on packages. In the following listing, you can see how to set these permissions and some examples of how to push a container image to the GitHub packages endpoint.

**Listing 8.12 Creating a container image and uploading to GitHub**

```yml
name: "chapter 08: create-container-and-push-frontend"
permissions:
  actions: write
  packages: write
  contents: read
  
on:
  push:
    branches: ["main"]
    paths:
    - 'frontend/**'
  workflow_dispatch:
  
jobs:
  build:
    uses: ./.github/workflows/create-container-and-push.yml
    with:
      imageRepository: 'frontend'
      containerRegistry: 'ghcr.io/githubactionsinaction'
      dockerfilePath: 'frontend/Dockerfile'
      namespace: 'globoticket'
    secrets:
      registryPassword: '${{ secrets.EXTENDED_ACCESSTOKEN }}'
```

Because we need to create a container image for every service we have in our application, we used a reusable workflow that actually builds the container. Listing 8.12 contains a reference to

```yml
uses: ./.github/workflows/create-container-and-push.yml
```

This refers to the reusable action workflow shown in the following listing.

**Listing 8.13 A reusable workflow that creates and pushes the container**

```yml
name: "chapter 08: create-container-and-push"
permissions:
  actions: write
  packages: write
  contents: read
on:
  #define the input parameters for this workflow used in the workflow call
  workflow_call:
    inputs:
      imageRepository:
        required: true
        type: string
      containerRegistry:
        required: true
        type: string
      dockerfilePath:
        required: true
        type: string
      namespace:
        required: true
        type: string
    secrets:
      registryPassword:
        required: true
  # the input parameters are also defined for a manual trigger
  workflow_dispatch:
    inputs:
      imageRepository:
        required: true
        type: string
        default: 'frontend'
      containerRegistry:
        required: true
        type: string
        default: 'ghcr.io/vriesmarcel'
      dockerfilePath:
        required: true
        type: string
        default: 'frontend/Dockerfile'
      namespace:
        required: true
        type: string
        default: 'globoticket'
jobs:
  build:
    # we check out the sources, determine the version number and
# login to the container registry  
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
  
      - name: Install GitVersion
        uses: gittools/actions/gitversion/setup@v0.10.2
        with:
          versionSpec: '6.x'
  
      - name: Determine Version
        id: gitversion
        uses: gittools/actions/gitversion/execute@v0.10.2
   
      - name: Login to GitHUb
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.registryPassword }}
      # we use docker buildx to create a builder instance and then
# build and push the image
      - name: select docker driver
        run: |
          docker buildx create --use --driver=docker-container
      # we use this action to determine the labels for the image
      - name: Docker meta
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: actions-with-actions/globoticket
      # build and push the image to the container registry
      - name: Build and push
        uses: docker/build-push-action@v6
        with:
            context: ${{github.workspace}}
            file: ${{inputs.dockerfilePath}}
            push: true
            tags: ${{inputs.containerRegistry}}/${{inputs.imageRepository}}:${{env.GitVersion_SemVer}}
            cache-from: type=gha
            cache-to: type=gha,mode=max
            labels: ${{steps.meta.outputs.labels}}
```

You can see in the reusable action workflow that we push the resulting artifact to the GitHub Artifact Registry. You can do this in a similar way if you are pushing packages from any of the supported package managers. When pushing a package, you also use the GitHub token to authenticate against the package registry.

Linking the package to the repo

It is important to note that you need to link the package that you publish to the repository. Linking it back to the source repository enables it to also send events that you can use to trigger (e.g., the release and deployment). You can enable this by creating the link in GitHub Postal, using the page you find when you look for the details of the package (see figure 8.8).

Alternatively, you can also enable this link back to the source repository by providing the metadata during publication on the Docker push action or adding the label to the Docker image when you build it

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH08_F08_Kaufmann.png)<br>
**Figure 8.8 Linking the package to the repo**

### 8.5.6 Using the upload/download capability to store artifacts

In case you are not using container images or packages and have a set of binaries or a zip file that you want to retain as part of your CI workflow, you can use an action called `actions/upload-artifact`. This action can take any set of arbitrary files and upload them to GitHub. Another workflow can then retrieve these files using the `actions/download-artifact` action.

When creating artifacts to deploy our sample application to a Kubernetes cluster, we need to produce a deployment descriptor file that references the newly created container during our CI. One way to do this is by using an action that can annotate an existing file you have in your repository and then outputting the altered results as an artifact we are going to store on GitHub. This can then be retrieved by the deployment workflow later. The following listing shows a simple example of a workflow storing a file and retrieving it in a second job.

**Listing 8.14 Uploading artifacts to GitHub**

```yml
name: Upload and Download arbitrary artifacts
on:
  workflow_dispatch:
env:
  deploymentFile: 'file-I-want-to-use-in-deploy-phase.txt'
jobs:
  
  build:
  
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
  
    - name: create a file we will use in next job
      run: |
        touch ${{github.workspace}}/${{env.deploymentFile}}
        
    - name: Upload a Build Artifact
      uses: actions/upload-artifact@v3
      with:
        name: deployfile
        path: ${{github.workspace}}/${{env.deploymentFile}}
        
  deploy:
    runs-on: ubuntu-latest
    needs: build
    steps:
    - name: Download artifact from build job
      uses: actions/download-artifact@v3
      with:
        name: deployfile
    - name: show files downloaded
      run: |
        ls  ${{github.workspace}}
```

The result of this workflow is that we’ve uploaded a file. We can see this result in a second job that was started with the name `deploy`. You can see the artifacts you create in the UI, as shown in figure 8.9.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH08_F09_Kaufmann.png)<br>
**Figure 8.9 Artifact publishing**

### 8.5.7 Preparing deployment artifacts

When you release your software, you want to get a fully prepared package that you can deploy. In our example, we need not only a set of containers in the container registry, but we also need a set of files that we use to run the deployment to the Kubernetes cluster. These are deployment files that contain a reference to the image we want to run.

To ensure you have a complete package that is traceable to the source and changes, the best practice is to prepare the deployment files as part of the CI workflow. In the case of the deployment of GloboTicket, this means we take the Kubernetes deployment file that we use as a template for the deployment and replace variables in this template file. After creating the containers and scanning them for known vulnerabilities, we then create the deployment file with the tags that were created while building the containers. After the replacement of the variables in the template, we can make this part of the artifacts that get pushed to the repo to be picked up by another workflow.

To transform existing files, we use the action `cschleiden/replace-tokens`. This action has the option to specify a replacement token and then replace this across a set of files. The example here is the tag of the container that will get pulled by Kubernetes with the tag created while creating the container. The following listing shows how to prepare a Kubernetes deployment file.

**Listing 8.15 Kubernetes template deployment file**

```yml
apiVersion: apps/v1
# Kubernetes deployment specification. We want to deploy our
# container frontend to the cluster, in the namespace globoticket.
kind: Deployment
metadata:
  name: frontend
  namespace: globoticket
  labels:
    app: frontend
# We want to deploy 3 replicas of the frontend and deploy
# them with a rolling update strategy
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2
      maxUnavailable: 0
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
```

Here is the specification of the container we want to deploy. We set the resource limits and requests according to best practices, and we pull the image from the GitHub container registry, using the secrets defined in the `pullsecret`.

```yml
    spec:
      containers:
      - name: frontend
        image: ghcr.io/vriesmarcel/frontend:#{Build.version}#
        resources:
          requests:
            memory: "500Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: «750m»
        env:
        - name: ASPNETCORE_ENVIRONMENT
          value: Development
        - name: ApiConfigs__EventsCatalog__Uri
          value: http://catalog:8080
        - name: ApiConfigs__Ordering__Uri
          value: http://ordering:8080
        ports:
        - containerPort: 80
        imagePullPolicy: Always
      imagePullSecrets:
        - name: pullsecret
```

When you look at the file, you see the markers that can be used to replace. In this case, I am going to replace the part that states `#{Build.version}#` with the number of the build that we are running. This is the same as the number we generate when we create the image and push the image to the registry. By ensuring these numbers are the same, you guarantee that you deploy exactly those images.

Replacement can now be done by pointing to this file and defining the replacement tokens and the variable for `Build.version`. The way to do this is shown in the following listing.

**Listing 8.16 Replacing tokens**

```yml
      - name: Replace tokens
        uses: cschleiden/replace-tokens@v1.0
        with:
          files: '["${{github.workspace}}/${{env.deploymentfileFolder}}
                          /frontend.yaml"]'
        env: 
          Build.version: ${{env.FRONTED_VERSION}}
```

After replacing the tokens in the file, we upload them to the artifact store, as described in the previous paragraph, so they can be retrieved the moment we want to run the deployment.

### 8.5.8 Creating a release

Creating a release is the starting point of moving the created deployment artifacts to the outside world. It is the hand off to the CD workflow that does the actual deployment. The deployment artifacts can be a set of packages that are going to be published, like a set of container images to be pulled from a container registry.

You can create a release in GitHub by using the create release page in GitHub. When using this page, you are doing it manually, which can be a good practice if you want to separate duties of people who can create releases from those who cannot. This release defines what we want to release, and we prefer to add all artifacts that we deploy to this release.

It is a best practice to create a release using an action in the CI workflow. When using a branching strategy like GitHub flow or Trunk-based development, you create a new release the moment you merged a change into the main branch. The main branch is the source to release to the production environments. This is normally done via a pull request that is merged, helping you to ensure compliance by providing good traceability and adherence to the four-eyes principle before something can move to a production environment.

You can define that regardless of how the change moved to the main branch. The moment we detect a change, we first want to trigger the CI workflows. After all of them have completed and are successful, we want to create a release that, in its turn, will trigger the CD workflow that moves the software to a production environment, with the necessary steps based on the process you want to follow.

You can trigger the release (e.g., the moment a new container image is published) via one of the previous workflows. Listing 8.17 shows the workflow that is triggered by the publication of the container image, picks up the version number from the image, and produces a file that is used for deployment to the Kubernetes cluster. This file is attached as an artifact that is part of the release, so it can be used by the CD workflow we discuss in the next chapter.

**Listing 8.17 Creating a release automatically**

```yml
name: "chapter 08: create release"
permissions:
  actions: write
  packages: write
  contents: read
on:
  registry_package: 
    types: [published]
  
env:
  deploymentFolder: 'deployment-automation'
  GH_TOKEN: ${{ secrets.EXTENDED_ACCESSTOKEN  }} #required for gh tool
```

Only run this workflow when a package is published with a tag that is not empty. We cancel any other releases that are in progress before we create the GitHub release. We need the latest version of the images; we use these to patch the deployment files with the correct versions and then create a release with the version provided by the package push:

```yml
jobs:
  release:
    if: github.event.registry_package.package_version.container_metadata.tag.name != ''
    concurrency:
      group: ${{github.event.registry_package.package_version.container_metadata.tag.name}}
      cancel-in-progress: true
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
```

Get the versions of the images from the package registry:

```yml
      - name: Retrieve latest image version frontend 
        run: |
          export FRONTED_VERSION=
            $(gh api user/packages/container/frontend/versions |
                    jq -r '.[0].metadata.container.tags[0]') 
          echo "FRONTED_VERSION=$FRONTED_VERSION" >> $GITHUB_ENV
          export ORDERING_VERSION=
            $(gh api user/packages/container/ordering/versions | 
                    jq -r '.[0].metadata.container.tags[0]')
          echo "ORDERING_VERSION=$ORDERING_VERSION" >> $GITHUB_ENV
          export CATALOG_VERSION=
            $(gh api user/packages/container/catalog/versions |
                    jq -r '.[0].metadata.container.tags[0]')   
          echo «CATALOG_VERSION=$CATALOG_VERSION» >> $GITHUB_ENV
```

Patch the deployment files with the correct versions. We do this for the catalog, frontend, and ordering:

```yml
      - name: Replace tokens
        uses: cschleiden/replace-tokens@v1.0
        with:
          files: '["${{github.workspace}}/${{env.deploymentFolder}}/catalog.yaml"]'
        env: 
          Build.version: ${{env.CATALOG_VERSION}}
  
      - name: Replace tokens
        uses: cschleiden/replace-tokens@v1.0
        with:
          files: '["${{github.workspace}}/${{env.deploymentFolder}}/frontend.yaml"]'
        env: 
          Build.version: ${{env.FRONTED_VERSION}}
  
      - name: Replace tokens
        uses: cschleiden/replace-tokens@v1.0
        with:
          files: '["${{github.workspace}}/${{env.deploymentFolder}}/ordering.yaml"]'
        env: 
          Build.version: ${{env.ORDERING_VERSION}}
```

Create a release with the version provided by the package push that contains the deployment files:

```yml
      - name: create a relase with version provided by package push
        uses: softprops/action-gh-release@v1
        with:
          token: "${{ secrets.EXTENDED_ACCESSTOKEN }}"
          tag_name: "v${{github.event.registry_package.package_version.container_metadata.tag.name}}"
          generate_release_notes: true
          files: | ${{github.workspace}}/${{env.deploymentfileFolder}}/frontend.yaml
              ${{github.workspace}}/${{env.deploymentfileFolder}}/ordering.yaml
              ${{github.workspace}}/${{env.deploymentfileFolder}}/catalog.yaml
```

After running this workflow, you will find the release in GitHub’s Releases section, and an event will be generated to signal a new release has been created. It is also possible to use the GitHub API to add files to the release—for example, attaching the SBOM discussed in section 8.5.2.

This is also the reason this workflow uses a different token than the standard GitHub token available in the workflow. If we use the default token, the release will not trigger any new workflows that could, for example, take care of the deployment. The token stored in GitHub secrets provides the ability to trigger a new workflow as part of the publication process.

This way of working ensures you have a very clear and simple workflow with a focus on creating the CI end result: a release. Now, it becomes more maintainable and can be secured in terms of who is allowed to review the change before acceptance. It is a good practice to upload all files you need as part of the deployment process. That way, the release becomes the container of all deliverables you need to execute a release and, hence, the perfect hand off to the CD workflows we cover in the next chapter.

## 8.6 The CI workflows for GloboTicket

Now that we have the concepts in place, let’s start creating the CI workflows we need to get our GloboTicket application ready for deployment. GloboTicket has two APIs and one frontend web application that needs to get deployed. If we take this application and design the CI workflows, we will need the following:

* One workflow to validate the integration on each pull request
* The same workflow that validates the integration in the main branch
* One workflow that tests the APIs or the frontend application, using the available unit tests
* One workflow that checks for known vulnerabilities in the committed sources and the dependencies in use
* One workflow that creates the artifacts ready for the deployment to a Kubernetes cluster

Let us go through these workflows one by one, so you get a full end-to-end view on how we can prepare everything for deployment to the cloud.

### 8.6.1 The integration CI for APIs and frontends

This workflow will trigger the moment we commit a change to any feature branch. The first step is to get the sources and then we use the `dotnet` tools to compile the sources. This workflow only compiles the source, so we know that what we committed integrates and compiles. This way, we get feedback as quickly as possible to the developer, who is building a new feature on a feature branch. The action workflow for this CI is shown in the following listing.

**Listing 8.18 Compiling and testing feedbac**k

```yml
name: "chapter 08: Compile and Test fast feedback"
permissions:
  actions: write
  contents: read
  
on:
  workflow_dispatch:
  push:
    branches: [ "feature/*" ]
    paths:
    - 'frontend/**'
    - 'catalog/**'
    - 'ordering/**'
  
jobs:
  build:
 
    runs-on: ubuntu-latest
  
    steps:
    - uses: actions/checkout@v4
    - name: Setup .NET
      uses: actions/setup-dotnet@v4
      with:
        dotnet-version: 6.0.x
      
    - name: Restore dependencies
      run: dotnet restore
    - name: Build
      run: dotnet build --no-restore
```

### 8.6.2 CI workflows for quality control

This kind of workflow aims to check if the software is still working according to requirements. This is validated by running the unit test projects that are part of the project. In `dotnet`, this involves using the built-in test tools. To get the right test results in the output, it is possible to use a specific logger that can produce markdown output. This output file can then be output to the step results, so it shows up in the final report. This way, you get a nice report that is visible in the GitHub user interface. After you run the workflow, you will see the results, like the ones shown in figure 8.10.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH08_F10_Kaufmann.png)<br>
**Figure 8.10 Test results summary**

The workflow for quality control on GloboTicket is shown in listing 8.19. We run this workflow the moment we create a pull request. This provides input to the team of reviewers and the developer of the feature regarding the current state of the feature. It is fine to combine the first CI workflow with this one, when the unit tests provide fast feedback. At the moment, this takes several minutes, in which case it makes more sense to split them.

**Listing 8.19 Adding a test results summary**

```yml
name: Compile and Test --fast feedback
permissions:
  actions: write
  contents: read
env:
  GH_TOKEN: ${{ github.token }}
  
on:
  workflow_dispatch:
  
  pull_request:
    branches: [ «main» ]
    paths:
    - 'frontend/**'
    - 'catalog/**'
    - 'ordering/**'
jobs:
  build:
    runs-on: ubuntu-latest
  
    steps:
    - uses: actions/checkout@v4
  
    - name: Setup .NET
      uses: actions/setup-dotnet@v4
      with:
        dotnet-version: 6.0.x
  
    - name: add markdown report logger for frontend project
      run: dotnet add unittests/unittests.csproj  package LiquidTestReports.Markdown
      
    - name: Test
      run: dotnet test --logger "liquid.md;logfilename=testResults.md" 
  
    - name: Output the results to the actions jobsummary
      if: always()
      run: cat $(find . -name testResults.md) >> $GITHUB_STEP_SUMMARY
```

### 8.6.3 The CI workflow for security testing

This workflow aims to periodically check the software on the main branch as well as at the moment we push changes. The software will be checked for known vulnerabilities produced by the development team. This is done using the GitHub Advanced Security scanning tool, which is reported back to the GitHub security dashboard in the UI. You can access this tool by activating Advanced Security in the Account section and selecting the setup CodeQL Analysis (see figure 8.11).

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH08_F11_Kaufmann.png)<br>
**Figure 8.11 GitHub Advanced Security**

When you run this workflow, you will see that CodeQL Analysis finds four known vulnerabilities in the code we have for GloboTicket—all with a severity of High! You can see the results in figure 8.12.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH08_F12_Kaufmann.png)<br>
**Figure 8.12 Code scanning results**

After scanning for known vulnerabilities in the code, the next step is to scan for known vulnerabilities in the container images. To achieve this, the workflow determines the latest version of the container images available and then runs the tools from section 8.5.4. We can also extend this to use the same GitHub Security Dashboard, by configuring the Trivy security scanner to output a static analysis results interchange format (SARIF) and then uploading this to GitHub. SARIF is an OASIS standard that defines an output file format. The SARIF standard is used to streamline how static analysis tools share their results.

This workflow will find multiple known vulnerabilities in the container images. Solving these vulnerabilities is easy to mitigate by changing the default base images used for .NET core containers to Alpine instead of Ubuntu. The result of the workflow will show up in the code scanning results, as shown in figure 8.12.

The workflow for security is also triggered on a pull request, since it takes some more time to complete. It is also set up to run on the main branch when there is a push and on a regular schedule, so we always keep an eye on potential new vulnerabilities. The code for the workflow is shown in the following listing.

**Listing 8.20 Security testing**

```yml
name: "chapter 08: Security Testing"
 
env:
  imageRepository: 'frontend'
  containerRegistry: 'ghcr.io/xpiritcommunityevents'
  dockerfilePath: 'frontend/Dockerfile'
  
on:
  workflow_dispatch:
  
jobs:
```

Run the `codeql` analysis on the code. We use a matrix to run the analysis on multiple languages, and we define the languages `c#` and `javascript`:

```yml
  analyzecode:
    name: Analyze
    runs-on: ${{ (matrix.language == 'swift' && 'macos-latest') || 'ubuntu-latest' }}
    timeout-minutes: ${{ (matrix.language == 'swift' && 120) || 360 }}
    permissions:
      actions: read
      contents: read
      security-events: write
  
    strategy:
      fail-fast: false
      matrix:
        language: [ 'csharp', 'javascript' ]
  
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
  
    - name: Initialize CodeQL
      uses: github/codeql-action/init@v2
      with:
        languages: ${{ matrix.language }}
    - name: Autobuild
      uses: github/codeql-action/autobuild@v2
  
    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v2
      with:
        category: "/language:${{matrix.language}}"
```

Next, we run the Trivy vulnerability scanner on our container images. This way, we can find vulnerabilities in our container images. We determine the latest version of the images and use that version to scan. This is done using the GitVersion tool.

```yml
  analyzecontainers:    
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write
      packages: read
  
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with: 
          fetch-depth: 0
      # determine the version of the image
      - name: Install GitVersion 
        uses: gittools/actions/gitversion/setup@v0.9.7
        with: 
          versionSpec: '5.x'
  
      - name: Determine Version 
        id: gitversion 
        uses: gittools/actions/gitversion/execute@v0.9.7 
      
        # use trivy to scan the container image
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{env.containerRegistry}}/${{env.imageRepository}}:${{env.GitVersion_SemVer}}
          format: 'sarif'
          output: 'trivy-results.sarif'
        env:
          TRIVY_USERNAME:  ${{ github.actor }}
          TRIVY_PASSWORD: ${{ secrets.GITHUB_TOKEN }}
```
      
Upload the results to the Security tab in GitHub. This is the same place the CodeQL results are uploaded:

```yml
      - name: Upload Trivy scan results to GitHub Security tab
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif' 
```      

### 8.6.4 The CI workflows for container image creation and publishing

This workflow gets triggered the moment the sources are pushed to the main branch. It will only create and push the new images to the registry, and we let the container registry trigger the creation of a new release. When you aren’t using containers, this would also be the workflow that would create the release immediately after the creation of the artifacts, and it would store those in the release to be used for deployment. The workflow that creates and publishes the containers uses the reusable workflow defined in listing 8.12. Listing 8.13 provides the YAML to create the container.

### 8.6.5 Creating a release

We can use the moment the container images are published as a trigger to create a release. We create the Kubernetes deployment files that go with the release. We need to determine what version numbers the various containers have at the container registry and use the correct version numbers in the deployment descriptor files needed at deployment. It will pick up the version numbers from the container images that got published so that is all in sync. It also publishes the deployment file as an artifact of the release, which can be used during deployment. The code for the YAML is shown in listing 8.17.

## 8.7 Conclusion

In this chapter, we started by describing the goals of CI. We defined the types of integration workflows we typically use and described how you can split up your CI workflows. We used the GitHub Flow branching strategy and created small action workflows, each with very specific tasks that triggered on specific points in the GitHub Flow process. The overall structure is shown in figure 8.13.

![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781633437302/files/OEBPS/Images/CH08_F13_Kaufmann.png)<br>
**Figure 8.13 GitHub Flow and the various workflow triggers**

In our workflow, we start with a feature branch, where we run the workflow that provides fast feedback, by compiling the sources committed to the branch. The moment we think we are ready to move the changes to main, we create a pull request, on which we trigger a set of workflows that help us determine the quality of the changes—not only from a testing perspective but also from the perspective of security. Once these quality checks are done, we can accept the pull request, and at that moment, the workflow that will create a set of container images is triggered. Once triggered, the container images get pushed to the container registry provided by GitHub.

When finished, this publication triggers a release. This release is versioned according to the version numbers the containers have, and the release contains the artifacts necessary to deploy the next phase. On its turn, this release can trigger a new workflow that supports CD. This process is described in the next chapter.

## Summary

* When your branching strategy and action workflows are aligned, you get a clear sense of the purpose of each workflow and a very clean way of handling the CI process.

* Each workflow should have a specific purpose, like integration, quality control, security testing, and packaging.

* The CI workflows are there to provide fast feedback on integration and code quality. The final step in CI is to package up the artifacts for CD. The most appropriate hand off mechanism in GitHub is to use the release and package the artifacts for deployment as part of the release.

* Artifacts are stored as part of a workflow execution and can be made part of a release. The latter is a great moment to hand off to the release and provide a version number.

* For GloboTicket, we created container images and pushed them to the container registry. We also created deployment descriptors that are used to deploy the containers to the Kubernetes cluster. These files are created and stored in the release.

[1]: https://github.com/GitHubActionsInAction/Globoticket
[2]: https://www.doi.org/10.1109/CMPSAC.1989.65147
[3]: https://agilemanifesto.org/iso/af/manifesto.html
[4]: http://agilemanifesto.org/principles.html
[5]: https://github.com/actions/runner-images
[6]: https://msrc.microsoft.com/blog/2020/12/december-21st-2020-solorigate-resource-center/
[7]: https://www.nist.gov/itl/executive-order-14028-improving-nations-cybersecurity